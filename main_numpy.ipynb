{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import datasets, metrics\n",
    "import urllib\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(method_name, \n",
    "                   closure, \n",
    "                   X, \n",
    "                   y, \n",
    "                   X_test, \n",
    "                   y_test,\n",
    "                   batch_size=10, \n",
    "                   max_epochs=50, \n",
    "                   m = 0, \n",
    "                   num_restarts=10, \n",
    "                   verbose=False, \n",
    "                   seed=9513451,\n",
    "                   **kwargs):\n",
    "    '''Run an experiment with multiple restarts and compute basic statistics from the runs.'''\n",
    "    # set the experiment seed\n",
    "    print(\"Running Experiment:\")\n",
    "    np.random.seed(seed)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "    line_search_results = []\n",
    "    arg_dict = kwargs\n",
    "\n",
    "    n,d = X.shape\n",
    "    n_test = X_test.shape[0]\n",
    "    x_sum = np.zeros(d)\n",
    "\n",
    "    # do the restarts\n",
    "    for i in range(num_restarts):\n",
    "        if method_name==\"svrg\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_bb\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo(closure = closure, \n",
    "                                                                               batch_size = batch_size,\n",
    "                                                                               D = X,\n",
    "                                                                               labels = y,\n",
    "                                                                               n = n,\n",
    "                                                                               d = d,\n",
    "                                                                               c=c,\n",
    "                                                                               beta=beta,\n",
    "                                                                               max_iter_armijo=max_iter_armijo,\n",
    "                                                                               max_step_size=max_step_size,\n",
    "                                                                               reset_step_size=reset_step_size,\n",
    "                                                                               max_epoch=max_epochs,\n",
    "                                                                               m = m,\n",
    "                                                                               verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo_outer_end\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo_outer_end(closure = closure, \n",
    "                                                                                         batch_size = batch_size,\n",
    "                                                                                         D = X,\n",
    "                                                                                         labels = y,\n",
    "                                                                                         n = n,\n",
    "                                                                                         d = d,\n",
    "                                                                                         c=c,\n",
    "                                                                                         beta=beta,\n",
    "                                                                                         max_iter_armijo=max_iter_armijo,\n",
    "                                                                                         max_step_size=max_step_size,\n",
    "                                                                                         reset_step_size=reset_step_size,\n",
    "                                                                                         max_epoch=max_epochs,\n",
    "                                                                                         m = m,\n",
    "                                                                                         verbose=verbose)\n",
    "        elif method_name == \"svrg_armijo_outer_beginning\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo_outer_beginning(closure = closure, \n",
    "                                                                                               batch_size = batch_size,\n",
    "                                                                                               D = X,\n",
    "                                                                                               labels = y,\n",
    "                                                                                               n = n,\n",
    "                                                                                               d = d,\n",
    "                                                                                               c=c,\n",
    "                                                                                               beta=beta,\n",
    "                                                                                               max_iter_armijo=max_iter_armijo,\n",
    "                                                                                               max_step_size=max_step_size,\n",
    "                                                                                               reset_step_size=reset_step_size,\n",
    "                                                                                               max_epoch=max_epochs,\n",
    "                                                                                               m = m,\n",
    "                                                                                               verbose=verbose)\n",
    "        elif method_name == \"svrg_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            adaptive_termination = arg_dict[\"adaptive_termination\"]            \n",
    "            x, loss_record, gradnorm_record = svrg_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose, adaptive_termination = adaptive_termination)   \n",
    "            \n",
    "        elif method_name == \"svrg_bb_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Method does not exist')\n",
    "            \n",
    "        x_sum += x\n",
    "        loss_results.append(loss_record)\n",
    "        gradnorm_results.append(gradnorm_record)\n",
    "\n",
    "        if verbose:\n",
    "            y_predict = np.sign(np.dot(X_test, x))\n",
    "            print('Restart %d, Test accuracy: %f' % (i, (np.count_nonzero(y_test == y_predict)*1.0 / n_test)))\n",
    "\n",
    "    # compute basic statistics from the runs\n",
    "    x_mean = x_sum / num_restarts\n",
    "\n",
    "    loss_results = np.stack(loss_results)\n",
    "    loss_std = loss_results.std(axis=0)\n",
    "    loss_mean = loss_results.mean(axis=0)\n",
    "\n",
    "    gradnorm_results = np.stack(gradnorm_results)\n",
    "    gradnorm_std = gradnorm_results.std(axis=0)\n",
    "    gradnorm_mean = gradnorm_results.mean(axis=0)\n",
    "\n",
    "    if method_name in [\"svrg_armijo\", \"svrg_armijo_outer_end\", \"svrg_armijo_outer_beginning\"]:\n",
    "        line_search_results.append(line_search_records)\n",
    "        line_search_results = np.stack(line_search_results)\n",
    "        line_search_std = line_search_results.std(axis=0)\n",
    "        line_search_mean = line_search_results.mean(axis=0)\n",
    "        return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std, line_search_mean, line_search_std\n",
    "    else:                           \n",
    "        return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closure(loss_fn, prior_prec=1e-2):\n",
    "    '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            loss_fn: the loss function to use (logistic loss, hinge loss, squared error, etc)\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: a closure fn for computing the loss and gradient. '''\n",
    "\n",
    "    def closure(w, X, y, backwards = True):\n",
    "        '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            w: weight vector\n",
    "            X: minibatch of input vectors\n",
    "            y: labels for the input vectors\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: (loss, gradient)'''\n",
    "        # change the Numpy Arrays into PyTorch Tensors\n",
    "        X = torch.tensor(X)\n",
    "        # Type of X is double, so y must be double.\n",
    "        y = torch.tensor(y, dtype=torch.double)\n",
    "        w = torch.tensor(w, requires_grad=True)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(w, X, y) + (prior_prec / 2) * torch.sum(w**2)\n",
    "        \n",
    "        if backwards:\n",
    "            # compute the gradient of loss w.r.t. w.\n",
    "            loss.backward()\n",
    "            # Put the gradient and loss back into Numpy.\n",
    "            grad = w.grad.detach().numpy()\n",
    "            loss = loss.item()\n",
    "\n",
    "            return loss, grad\n",
    "        else:\n",
    "            loss = loss.item()\n",
    "            \n",
    "            return loss\n",
    "\n",
    "    return closure\n",
    "\n",
    "# PyTorch Loss Functions\n",
    "\n",
    "def logistic_loss(w, X, y):\n",
    "    ''' Logistic Loss'''\n",
    "    n,d = X.shape\n",
    "    return torch.mean(torch.log(1 + torch.exp(-torch.mul(y, torch.matmul(X, w)))))\n",
    "\n",
    "def squared_hinge_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Hinge Loss '''\n",
    "    return torch.mean((torch.max( torch.zeros(n,dtype=torch.double) , torch.ones(n,dtype=torch.double) - torch.mul(y, torch.matmul(X, w))))**2 )\n",
    "\n",
    "def squared_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Loss'''\n",
    "    return torch.mean(( y - torch.matmul(X, w) )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBSVM_URL = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/\"\n",
    "LIBSVM_DOWNLOAD_FN = {\"rcv1\"       : \"rcv1_train.binary.bz2\",\n",
    "                      \"mushrooms\"  : \"mushrooms\",\n",
    "                      \"a1a\"  : \"a1a\",\n",
    "                      \"a2a\"  : \"a2a\",\n",
    "                      \"ijcnn\"      : \"ijcnn1.tr.bz2\",\n",
    "                      \"w8a\"        : \"w8a\"}\n",
    "\n",
    "\n",
    "\n",
    "def load_libsvm(name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    fn = LIBSVM_DOWNLOAD_FN[name]\n",
    "    data_path = os.path.join(data_dir, fn)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        url = urllib.parse.urljoin(LIBSVM_URL, fn)\n",
    "        print(\"Downloading from %s\" % url)\n",
    "        urllib.request.urlretrieve(url, data_path)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    X, y = load_svmlight_file(data_path)\n",
    "    return [X, y]\n",
    "\n",
    "def data_load(data_dir, dataset_num, n = 0, d = 0, margin = 1e-6, false_ratio = 0, is_subsample = 0, is_kernelize = 0, test_prop=0.2, split_seed=9513451):\n",
    "\n",
    "    if dataset_num == 0:\n",
    "        data_name = 'quantum'\n",
    "\n",
    "    elif dataset_num == 1:\n",
    "        data_name = 'rcv1'\n",
    "\n",
    "    elif dataset_num == 2:\n",
    "        data_name = 'protein'\n",
    "\n",
    "    elif dataset_num == 3:\n",
    "        data_name = 'news'\n",
    "\n",
    "    elif dataset_num == 4:\n",
    "        data_name = 'mushrooms'\n",
    "\n",
    "    elif dataset_num == 5:\n",
    "        data_name = 'splice'\n",
    "\n",
    "    elif dataset_num == 6:\n",
    "        data_name = 'ijcnn'\n",
    "\n",
    "    elif dataset_num == 7:\n",
    "        data_name = 'w8a'\n",
    "\n",
    "    elif dataset_num == 8:\n",
    "        data_name = 'covtype'\n",
    "\n",
    "    elif dataset_num == -1:\n",
    "        data_name = 'synthetic'\n",
    "\n",
    "    if (dataset_num >= 0):\n",
    "\n",
    "        # real data\n",
    "#         data = pickle.load(open(data_dir + data_name +'.pkl', 'rb'), encoding = \"latin1\")\n",
    "        data = load_libsvm(data_name, data_dir='./')\n",
    "\n",
    "        # load real dataset\n",
    "        A = data[0].toarray()\n",
    "\n",
    "        if dataset_num < 4:\n",
    "            y = data[1].toarray().ravel()\n",
    "        else:\n",
    "            y = data[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        A, y, w_true = create_dataset(n,d, margin, false_ratio)\n",
    "\n",
    "        # generate synthetic data - according to the BB paper\n",
    "#         x = np.random.randn(n, d)\n",
    "#         w_true = np.random.randn(d)\n",
    "#         y = np.sign(np.dot(x, w_true))\n",
    "\n",
    "    # subsample\n",
    "    if is_subsample == 1:\n",
    "        A = A[:n,:]\n",
    "        y = y[:n]\n",
    "\n",
    "    # split dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(A, y, test_size=test_prop, random_state=split_seed)\n",
    "\n",
    "    if is_kernelize == 1:\n",
    "        # Form kernel\n",
    "        A_train, A_test = kernelize(X_train, X_test, dataset_num, data_dir=data_dir)\n",
    "    else:\n",
    "        A_train = X_train\n",
    "        A_test = X_test\n",
    "\n",
    "    print('Loaded ', data_name ,' dataset.')\n",
    "\n",
    "    return A_train, y_train, A_test, y_test\n",
    "\n",
    "def kernelize(X, X_test, dataset_num, kernel_type=0, data_dir=\"./Data\"):\n",
    "\n",
    "    n = X.shape[0]\n",
    "\n",
    "    fname = data_dir + '/Kernel_' + str(n) + '_' + str(dataset_num) + '.p'\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "\n",
    "        print('Reading file ', fname)\n",
    "        X_kernel, X_test_kernel = pickle.load( open( fname, \"rb\" ) )\n",
    "\n",
    "    else:\n",
    "        if kernel_type == 0:\n",
    "            X_kernel = RBF_kernel(X, X)\n",
    "            X_test_kernel = RBF_kernel(X_test, X)\n",
    "            print('Formed the kernel matrix')\n",
    "\n",
    "        pickle.dump( (X_kernel, X_test_kernel) , open( fname, \"wb\" ) )\n",
    "\n",
    "    return X_kernel, X_test_kernel\n",
    "\n",
    "def RBF_kernel( A, B, sigma = 1.0 ):\n",
    "\n",
    "    distance_2 = np.square(  metrics.pairwise.pairwise_distances( X = A, Y = B, metric='euclidean'  )   )\n",
    "    K = np.exp( -1 * np.divide( distance_2, (2 * (sigma**2)) )  )\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def create_dataset(n,d,gamma = 0, false_ratio = 0):\n",
    "# create synthetic dataset using the python utility\n",
    "# X, y = datasets.make_classification(n_samples=n, n_features=d,n_informative = d, n_redundant = 0, class_sep = 2.0 )\n",
    "# convert into -1/+1\n",
    "# y = 2 * y - 1\n",
    "\n",
    "# create linearly separable dataset with margin gamma\n",
    "#w_star = np.random.random((d,1))\n",
    "    w_star = np.random.normal(0,1,(d,1))\n",
    "# normalize w_star\n",
    "    w_star = w_star / np.linalg.norm(w_star)\n",
    "\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    count = 0\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    y = np.zeros((n))\n",
    "\n",
    "    while(1):\n",
    "\n",
    "        x = np.random.normal( 1,1,(1,d) ) \n",
    "        # normalize x s.t. || x ||_2 = 1\n",
    "#         x = x / np.linalg.norm(x)\n",
    "\n",
    "        temp = np.dot( x, w_star )\n",
    "        margin = abs( temp )\n",
    "        sig = np.sign( temp )\n",
    "\n",
    "        if margin > gamma * np.linalg.norm(w_star):\n",
    "\n",
    "            if count % 2 == 0:\n",
    "\n",
    "                # generate positive\n",
    "                if sig > 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count, :] = -x\n",
    "                y[ count ] = + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                # generate negative\n",
    "                if sig < 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count,:] = -x\n",
    "                y[ count ] = - 1\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        if count == n:\n",
    "            break\n",
    "            \n",
    "    flip_ind = np.random.choice(n, int(n*false_ratio))\n",
    "    y[flip_ind] = -y[flip_ind]\n",
    "    \n",
    "    return X, y, w_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_minibatches(n, m, minibatch_size):\n",
    "    ''' Create m minibatches from the training set by sampling without replacement.\n",
    "        This function may sample the training set multiple times.\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        m: number of minibatches to generate\n",
    "        batch_size: size of the desired minibatches'''\n",
    "\n",
    "    k = math.ceil(m * minibatch_size / n)\n",
    "    batches = []\n",
    "    for i in range(k):\n",
    "        batches += minibatch_data(n, minibatch_size)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def minibatch_data(n, batch_size):\n",
    "    '''Splits training set into minibatches by sampling **without** replacement.\n",
    "    This isn't performant for large datasets (e.g. we should switch to PyTorch's streaming data loader eventually).\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        batch_size: size of the desired minibatches'''\n",
    "    # shuffle training set indices before forming minibatches\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    batches = []\n",
    "    num_batches = math.floor(n / batch_size)\n",
    "    # split the training set into minibatches\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        stop_index = (batch_num + 1) * batch_size\n",
    "\n",
    "        # create a minibatch\n",
    "        batches.append(indices[start_index:stop_index])\n",
    "\n",
    "    # generate a final, smaller batch if the batch_size doesn't divide evenly into n\n",
    "    if num_batches != math.ceil(n / batch_size):\n",
    "        batches.append(indices[stop_index:])\n",
    "\n",
    "    return batches\n",
    "\n",
    "def reset(model):\n",
    "    # reset the model\n",
    "    for param in model.parameters():\n",
    "        param.data = torch.zeros_like(param)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plotting(results, labels, max_epochs):\n",
    "    plt.figure()\n",
    "\n",
    "    offset = 0\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot( x, ( results[i,:] ), color = colors[i], label = labels[i] )\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_shaded_error_bars(results_mean, results_std, max_epochs, ylabel=\"Loss\", colors=None, labels=None):\n",
    "    fig = plt.figure()\n",
    "    offset = 0\n",
    "    if colors is None:\n",
    "        colors = ['r', 'b', 'g','k','cyan','lightgreen', 'm', 'y', 'chartreuse']\n",
    "    if labels is None:\n",
    "        labels = ['SVRG-BB', 'SVRG']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(results_mean.shape[0]):\n",
    "        plt.plot( x, ( results_mean[i,:] ), color = colors[i], label = labels[i] )\n",
    "        plt.fill_between(x, (results_mean[i,:] - results_std[i,:]), (results_mean[i,:] + results_std[i,:]), color=colors[i], alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    step_size = init_step_size\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            output += ', Num gradient evaluations: %d' % num_grad_evals\n",
    "            print(output)\n",
    "\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "        \n",
    "        \n",
    "        num_grad_evals = num_grad_evals + n\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "            \n",
    "            num_grad_evals = num_grad_evals + 1\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None, \n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size in the inner loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    AVERAGE_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "    MAX_ARMIJO_STEPS_REACHED = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        \n",
    "        start_step_size = max_step_size\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            f_x ,x_grad = closure(x, Di, labels_i)\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "            g = x_grad - x_tilde_grad + full_grad\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            \n",
    "            step_size = start_step_size\n",
    "            found = False\n",
    "            for j in range(max_iter_armijo):\n",
    "                if closure(x - step_size*g, Di, labels_i, backwards = False) > f_x - c*step_size*g_norm:\n",
    "                    step_size *= beta\n",
    "                    AVERAGE_ARMIJO_STEPS[k] += 1\n",
    "                else:\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if found:\n",
    "                x -= step_size * g\n",
    "                if not reset_step_size:\n",
    "                    start_step_size = step_size\n",
    "            else:\n",
    "                MAX_ARMIJO_STEPS_REACHED[k]+=1\n",
    "                x -= 1e-6*g\n",
    "        AVERAGE_ARMIJO_STEPS[k] /= m\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Avg armijo steps: %.2e, Nb max iteration reached: %.2e' % \\\n",
    "                     (k, AVERAGE_ARMIJO_STEPS[k], MAX_ARMIJO_STEPS_REACHED[k])\n",
    "            print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, AVERAGE_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo_outer_end(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size at the end of the outer loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    \n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    NB_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        \n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        norm_full_grad = np.linalg.norm(full_grad)\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                    (k, start_step_size, norm_full_grad)\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "        \n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = norm_full_grad\n",
    "        step_size = start_step_size\n",
    "        \n",
    "        found = False\n",
    "        for j in range(max_iter_armijo):\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                f_x ,x_grad = closure(x, Di, labels_i)\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "                g = x_grad - x_tilde_grad + full_grad\n",
    "                x -= step_size * g\n",
    "                \n",
    "            if closure(x, D, labels, backwards = False) > loss - c*step_size*norm_full_grad:\n",
    "                step_size = beta*step_size\n",
    "                x = x_tilde.copy()\n",
    "                NB_ARMIJO_STEPS[k]+=1\n",
    "            else:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            if not reset_step_size:\n",
    "                start_step_size = step_size\n",
    "        if not found:\n",
    "            x -= 1e-6*full_grad\n",
    "        \n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Nb of Armijo steps: %.2e' % \\\n",
    "                         (k, NB_ARMIJO_STEPS[k])\n",
    "            print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, NB_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_armijo_outer_beginning(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size at the beginning of the outer loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    \n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    NB_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        \n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        norm_full_grad = np.linalg.norm(full_grad)\n",
    "        \n",
    "        step_size = start_step_size\n",
    "        while closure(x - step_size*full_grad, D, labels, backwards = False) > loss - c*step_size*norm_full_grad:\n",
    "            step_size *=beta\n",
    "            NB_ARMIJO_STEPS[k] +=1\n",
    "            if NB_ARMIJO_STEPS[k] == max_iter_armijo:\n",
    "                step_size = 1e-6\n",
    "                break\n",
    "        if not reset_step_size:\n",
    "            start_step_size = step_size\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e, Nb of Armijo steps: %d' % \\\n",
    "                    (k, step_size, norm_full_grad, NB_ARMIJO_STEPS[k])\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "        \n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = norm_full_grad  \n",
    "        \n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, NB_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True, adaptive_termination = False):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        \n",
    "#         if k == 0:\n",
    "        \n",
    "#             Gk2 = 0\n",
    "#             step_size = init_step_size\n",
    "                        \n",
    "#             minibatches = make_minibatches(n, m, batch_size)\n",
    "#             for i in range(m):\n",
    "#                 # get the minibatch for this iteration\n",
    "#                 indices = minibatches[i]\n",
    "#                 Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "#                 # compute the gradients:\n",
    "#                 x_grad = closure(x, Di, labels_i)[1]\n",
    "\n",
    "#                 gk = x_grad \n",
    "\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "\n",
    "#                 x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "#                 num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "#                 if adaptive_termination == True:\n",
    "\n",
    "#                     if i == 0:\n",
    "#                         org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                     else:\n",
    "#                         temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                         # check condition to terminate inner loop\n",
    "#                         if temp/org < 1e-2:\n",
    "#                             print('Breaking from inner loop')                    \n",
    "#                             break                            \n",
    "        \n",
    "        if k >= 0:\n",
    "            print('Here')\n",
    "            loss, full_grad = closure(x, D, labels)\n",
    "            x_tilde = x.copy()\n",
    "\n",
    "            last_full_grad = full_grad\n",
    "            last_x_tilde = x_tilde\n",
    "\n",
    "            # initialize running sum of gradient norms\n",
    "            Gk2 = 0\n",
    "            step_size = init_step_size\n",
    "\n",
    "            if verbose:\n",
    "                output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                         (k, np.linalg.norm(full_grad))\n",
    "                output += ', Func. value: %e' % loss\n",
    "                output += ', Num gradient evaluations: %d' % num_grad_evals            \n",
    "                print(output)\n",
    "\n",
    "            if np.linalg.norm(full_grad) <= 1e-14:\n",
    "                return x, LOSS, GRAD_NORM\n",
    "\n",
    "            LOSS[k] = loss\n",
    "            GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "            num_grad_evals =  num_grad_evals  + n\n",
    "\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                x_grad = closure(x, Di, labels_i)[1]\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "                gk = x_grad - x_tilde_grad + full_grad\n",
    "\n",
    "                Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(x_grad) ** 2)\n",
    "\n",
    "                x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "                num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "                if adaptive_termination == True:\n",
    "\n",
    "                    if i == 0:\n",
    "                        org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                    else:\n",
    "                        temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                        # check condition to terminate inner loop\n",
    "                        if temp/org < 1e-2 and i >=  int(n / batch_size):\n",
    "                            print(temp/org)\n",
    "                            print('Breaking from inner loop')                                \n",
    "                            break\n",
    "            \n",
    "            \n",
    "    return x, LOSS, GRAD_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        \n",
    "        Gk2 = 0\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            gk = x_grad - x_tilde_grad + full_grad\n",
    "            \n",
    "            Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "                        \n",
    "            x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "dataset_num = -1\n",
    "data_dir = './'\n",
    "\n",
    "is_subsample = 0\n",
    "is_kernelize = 0\n",
    "subsampled_n = -1\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(6162647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization options\n",
    "max_epochs = 15\n",
    "num_restarts = 1\n",
    "batch_size = 10\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem size when generating synthetic data\n",
    "if dataset_num == -1:\n",
    "    n, d = 10000, 20\n",
    "    false_ratio = 0.25\n",
    "    margin = 1e-6\n",
    "    print(is_kernelize)\n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num,n, d, margin, false_ratio)\n",
    "else:\n",
    "    if is_subsample == 1:\n",
    "        n = subsampled_n\n",
    "    else:\n",
    "        n = 0\n",
    "    if is_kernelize == 1:\n",
    "        d = n\n",
    "    else:\n",
    "        d = 0\n",
    "        \n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num, n,d, false_ratio, is_subsample, is_kernelize)\n",
    "    \n",
    "    if n == 0:\n",
    "        n = A.shape[0]\n",
    "    \n",
    "    \n",
    "#define the regularized losses we will use\n",
    "logistic_closure_l2 = make_closure(logistic_loss, 1/n)\n",
    "squared_hinge_closure_l2 = make_closure(squared_hinge_loss, 1/n)\n",
    "squared_closure_l2 = make_closure(squared_loss, 1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment\n",
    "exp_num = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if exp_num == 0:\n",
    "    print('Testing SVRG-BB:')\n",
    "    init_eta = 1e-3\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] = run_experiment(svrg_bb, logistic_closure_no_l2, A, y, A_test, y_test, init_eta, batch_size, max_epochs, num_restarts, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 1:\n",
    "    print('Testing robustness to step-size for SVRG:')    \n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = 'svrg', closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 2:\n",
    "    print('Testing robustness to size of inner loop for SVRG:')   \n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    init_eta = 1e-2\n",
    "    inner_loop_list = [0.5 , 1, 2, 5, 10]\n",
    "\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(inner_loop_list)\n",
    "\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for inner_loop in inner_loop_list: \n",
    "        print('Begin to run SVRG with inner loop size = :', inner_loop)\n",
    "\n",
    "        m = int(inner_loop * n)\n",
    "\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = m, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(inner_loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 3:\n",
    "    \n",
    "    labels = []\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    step_sizes = [1e-4, 1e-2, 1, 100]\n",
    "        \n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))     \n",
    "        \n",
    "    for init_eta in step_sizes:     \n",
    "  \n",
    "        print('Testing SVRG-Ada with step-size:', init_eta)\n",
    "    \n",
    "        x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = squared_closure_l2, \\\n",
    "                           X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                           init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                           m = 0, num_restarts = num_restarts, verbose=verbose) \n",
    "    \n",
    "        labels.append(['SVRG-Ada-', str(init_eta)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 4:\n",
    "    print('Testing SVRG vs SVRG-Ada vs SVRG-BB wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-3, 1e-2, 1e-1]\n",
    "    colors = ['r', 'b', 'g','r','b','g','r', 'b', 'g']\n",
    "\n",
    "    num_variants = 3 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = squared_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "#         # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "        \n",
    "        \n",
    "        # SVRG BB\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('BB' + str(init_eta))\n",
    "        \n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose,  adaptive_termination = True)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 5:\n",
    "    print('Testing SVRG-BB vs SVRG-BB-Ada wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-2, 1e-1, 1]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = 2 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = logistic_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "        # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 6:\n",
    "    print('Testing SVRG-Armijo:')\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "\n",
    "    x_mean, results_mean2[0, :], results_std2[0, :], results2_mean2[0, :], results2_std2[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_armijo\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = n, num_restarts = num_restarts, verbose=True, init_step_size = 0.01, c=0.1, max_step_size = 0.1, \\\n",
    "                       beta = 0.7, max_iter_armijo=10, reset_step_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 7:\n",
    "    print(\"Testing SVRG-Armijo, SVRG-Armijo with start of inner loop LS, SVRG-Armijo with end of inner loop LS with logistic loss\")\n",
    "    batch_sizes = [10, 100, 1000]\n",
    "    c_values = [0.1, 0.5]\n",
    "    beta_values = [0.8]\n",
    "    max_step_sizes = [0.1, 1, 10]\n",
    "    reset_step_sizes = [True, False]\n",
    "    max_iter_armijo = 50\n",
    "    verbose = True\n",
    "    labels = []\n",
    "    \n",
    "    count = 0\n",
    "    num_variants = 3*len(batch_sizes)*len(c_values)*len(beta_values)*len(max_step_sizes)*len(reset_step_sizes)\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    results3_mean = np.zeros((num_variants, max_epochs))\n",
    "    results3_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for c in c_values:\n",
    "            for beta in beta_values:\n",
    "                for max_step_size in max_step_sizes:\n",
    "                    for reset_step_size in reset_step_sizes:\n",
    "                        print(\"Begin to run with batch_size: \" + str(batch_size) + \", c: \" + str(c)+\\\n",
    "                             \", beta: \" + str(beta) + \", max_step_size: \"+ str(max_step_size) +\\\n",
    "                              \", reset_step_size:\" + str(reset_step_size))\n",
    "                        \n",
    "                        \n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo\", closure = squared_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=verbose, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        label = \"svrg_armijo-batch_size:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "                        count+=1\n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo_outer_end\", closure = squared_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=verbose, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        label = \"svrg_armijo_outer_end-batch_size:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "                        \n",
    "                        count+=1\n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo_outer_beginning\", closure = squared_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=verbose, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        label = \"svrg_armijo_outer_beginning-batch_size:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size)\n",
    "                        labels.append(label)\n",
    "                        \n",
    "                        count+=1\n",
    "\n",
    "save = False\n",
    "if save:\n",
    "    save_dir = \"./.tmp/results_exp7_logistic\"\n",
    "    np.savetxt(save_dir + \"loss_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"loss_std.out\", results_std, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"grad_norm_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"grad_norm_std.out\", results_std, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"armijo_steps_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"armijo_steps_std.out\", results_std, delimiter= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './.tmp/results_exp7_logistic/'\n",
    "loss_mean = np.genfromtxt(save_dir + \"loss_mean.out\", delimiter= \",\")\n",
    "loss_std = np.genfromtxt(save_dir + \"loss_std.out\", delimiter= \",\")\n",
    "grad_norm_mean = np.genfromtxt(save_dir + \"grad_norm_mean.out\", delimiter= \",\")\n",
    "grad_norm_std = np.genfromtxt(save_dir + \"grad_norm_std.out\", delimiter= \",\")\n",
    "armijo_steps_mean = np.genfromtxt(save_dir + \"armijo_steps_mean.out\", delimiter= \",\")\n",
    "armijo_steps_std = np.genfromtxt(save_dir + \"armijo_steps_std.out\", delimiter= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot each experiment separately\n",
    "batch_sizes = [10, 100, 1000]\n",
    "c_values = [0.1, 0.5]\n",
    "beta_values = [0.8]\n",
    "max_step_sizes = [0.1, 1, 10]\n",
    "reset_step_sizes = [True, False]\n",
    "count = 0\n",
    "colors = [\"r\", \"g\", \"b\"]\n",
    "labels = [\"armijo\", \"armijo_outer_end\", \"armijo_outer_beginning\"]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "        for c in c_values:\n",
    "            for beta in beta_values:\n",
    "                for max_step_size in max_step_sizes:\n",
    "                    for reset_step_size in reset_step_sizes:\n",
    "                        print(\"Plot with batch_size: \" + str(batch_size) + \", c: \" + str(c)+\\\n",
    "                             \", beta: \" + str(beta) + \", max_step_size: \"+ str(max_step_size) +\\\n",
    "                              \", reset_step_size:\" + str(reset_step_size))\n",
    "                        \n",
    "                        fig = plot_shaded_error_bars(results_mean = np.log(loss_mean[count*3:count*3+3, :]), \n",
    "                                                     results_std = np.log(loss_std[count*3:count*3 + 3, :]), \n",
    "                                                     ylabel = 'Loss', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     colors = colors, \n",
    "                                                     labels = labels)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        fig = plot_shaded_error_bars(results_mean = np.log(grad_norm_mean[count*3:count*3+3, :]), \n",
    "                                                     results_std = np.log(grad_norm_std[count*3:count*3 + 3, :]), \n",
    "                                                     ylabel = 'Grad Norm', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     colors = colors, \n",
    "                                                     labels = labels)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        fig = plot_shaded_error_bars(results_mean = armijo_steps_mean[count*3:count*3+3, :], \n",
    "                                                     results_std = armijo_steps_std[count*3:count*3 + 3, :], \n",
    "                                                     ylabel = 'Nb Armijo Steps', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     colors = colors, \n",
    "                                                     labels = labels)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        count+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for different batch sizes, rest fixed\n",
    "count = 0\n",
    "labels = [\"armijo_10\", \"armijo_end_10\", \"armijo_beginning_10\",\n",
    "         \"armijo_100\", \"armijo_end_100\", \"armijo_beginning_100\",\n",
    "         \"armijo_1000\", \"armijo_end_1000\", \"armijo_beginning_1000\"]\n",
    "for c in c_values:\n",
    "    for beta in beta_values:\n",
    "        for max_step_size in max_step_sizes:\n",
    "            for reset_step_size in reset_step_sizes:\n",
    "                print(\"Plot with c: \" + str(c)+\\\n",
    "                             \", beta: \" + str(beta) + \", max_step_size: \"+ str(max_step_size) +\\\n",
    "                              \", reset_step_size:\" + str(reset_step_size))\n",
    "                \n",
    "                plotted_loss_mean = np.concatenate((loss_mean[count:count+3,:], loss_mean[36+count:39+count,:]), axis = 0)\n",
    "                plotted_loss_mean = np.concatenate((plotted_loss_mean, loss_mean[72+count:75+count, :]), axis = 0)\n",
    "                plotted_loss_std = np.concatenate((loss_std[count:count+3,:], loss_std[36+count:39+count,:]), axis = 0)\n",
    "                plotted_loss_std = np.concatenate((plotted_loss_std, loss_std[72+count:75+count, :]), axis = 0)\n",
    "                \n",
    "                plotted_grad_norm_mean = np.concatenate((grad_norm_mean[count:count+3,:], grad_norm_mean[36+count:39+count,:]), axis = 0)\n",
    "                plotted_grad_norm_mean = np.concatenate((plotted_grad_norm_mean, grad_norm_mean[72+count:75+count, :]), axis = 0)\n",
    "                plotted_grad_norm_std = np.concatenate((grad_norm_std[count:count+3,:], grad_norm_std[36+count:39+count,:]), axis = 0)\n",
    "                plotted_grad_norm_std = np.concatenate((plotted_grad_norm_std, grad_norm_std[72+count:75+count, :]), axis = 0)\n",
    "                \n",
    "                plotted_armijo_steps_mean = np.concatenate((armijo_steps_mean[count:count+3,:], armijo_steps_mean[36+count:39+count,:]), axis = 0)\n",
    "                plotted_armijo_steps_mean = np.concatenate((plotted_armijo_steps_mean, armijo_steps_mean[72+count:75+count, :]), axis = 0)\n",
    "                plotted_armijo_steps_std = np.concatenate((armijo_steps_std[count:count+3,:], armijo_steps_std[36+count:39+count,:]), axis = 0)\n",
    "                plotted_armijo_steps_std = np.concatenate((plotted_armijo_steps_std, armijo_steps_std[72+count:75+count, :]), axis = 0)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                fig = plot_shaded_error_bars(results_mean = np.log(plotted_loss_mean), \n",
    "                                                     results_std = np.log(plotted_loss_std), \n",
    "                                                     ylabel = 'Loss', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     labels = labels)\n",
    "                plt.show()\n",
    "                \n",
    "                fig = plot_shaded_error_bars(results_mean = np.log(plotted_grad_norm_mean), \n",
    "                                                     results_std = np.log(plotted_grad_norm_std), \n",
    "                                                     ylabel = 'Grad Norm', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     labels = labels)\n",
    "                plt.show()\n",
    "                \n",
    "                fig = plot_shaded_error_bars(results_mean = plotted_armijo_steps_mean, \n",
    "                                                     results_std = plotted_armijo_steps_std, \n",
    "                                                     ylabel = 'Nb Armijo Steps', \n",
    "                                                     max_epochs = max_epochs, \n",
    "                                                     labels = labels)\n",
    "                plt.show()\n",
    "                count+=3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
