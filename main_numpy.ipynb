{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import datasets, metrics\n",
    "\n",
    "import urllib\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(method_name, \n",
    "                   closure, \n",
    "                   X, \n",
    "                   y, \n",
    "                   X_test, \n",
    "                   y_test,\n",
    "                   batch_size=10, \n",
    "                   max_epochs=50, \n",
    "                   m = 0, \n",
    "                   num_restarts=10, \n",
    "                   verbose=False, \n",
    "                   seed=9513451,\n",
    "                   **kwargs):\n",
    "    '''Run an experiment with multiple restarts and compute basic statistics from the runs.'''\n",
    "    # set the experiment seed\n",
    "    print(\"Running Experiment:\")\n",
    "    np.random.seed(seed)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "    arg_dict = kwargs\n",
    "\n",
    "    n,d = X.shape\n",
    "    n_test = X_test.shape[0]\n",
    "    x_sum = np.zeros(d)\n",
    "\n",
    "    # do the restarts\n",
    "    for i in range(num_restarts):\n",
    "        if method_name==\"svrg\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_bb\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_armijo(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   c=c,\n",
    "                                                   beta=beta,\n",
    "                                                   max_iter_armijo=max_iter_armijo,\n",
    "                                                   max_step_size=max_step_size,\n",
    "                                                   reset_step_size=reset_step_size,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo_outer_end\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_armijo_outer_end(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   c=c,\n",
    "                                                   beta=beta,\n",
    "                                                   max_iter_armijo=max_iter_armijo,\n",
    "                                                   max_step_size=max_step_size,\n",
    "                                                   reset_step_size=reset_step_size,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)                        \n",
    "        elif method_name == \"svrg_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            adaptive_termination = arg_dict[\"adaptive_termination\"]            \n",
    "            x, loss_record, gradnorm_record = svrg_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose, adaptive_termination = adaptive_termination)   \n",
    "            \n",
    "        elif method_name == \"svrg_bb_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Method does not exist')\n",
    "            \n",
    "        x_sum += x\n",
    "        loss_results.append(loss_record)\n",
    "        gradnorm_results.append(gradnorm_record)\n",
    "\n",
    "        if verbose:\n",
    "            y_predict = np.sign(np.dot(X_test, x))\n",
    "            print('Restart %d, Test accuracy: %f' % (i, (np.count_nonzero(y_test == y_predict)*1.0 / n_test)))\n",
    "\n",
    "    # compute basic statistics from the runs\n",
    "    x_mean = x_sum / num_restarts\n",
    "\n",
    "    loss_results = np.stack(loss_results)\n",
    "    loss_std = loss_results.std(axis=0)\n",
    "    loss_mean = loss_results.mean(axis=0)\n",
    "\n",
    "    gradnorm_results = np.stack(gradnorm_results)\n",
    "    gradnorm_std = gradnorm_results.std(axis=0)\n",
    "    gradnorm_mean = gradnorm_results.mean(axis=0)\n",
    "\n",
    "    return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closure(loss_fn, prior_prec=1e-2):\n",
    "    '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            loss_fn: the loss function to use (logistic loss, hinge loss, squared error, etc)\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: a closure fn for computing the loss and gradient. '''\n",
    "\n",
    "    def closure(w, X, y):\n",
    "        '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            w: weight vector\n",
    "            X: minibatch of input vectors\n",
    "            y: labels for the input vectors\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: (loss, gradient)'''\n",
    "        # change the Numpy Arrays into PyTorch Tensors\n",
    "        X = torch.tensor(X)\n",
    "        # Type of X is double, so y must be double.\n",
    "        y = torch.tensor(y, dtype=torch.double)\n",
    "        w = torch.tensor(w, requires_grad=True)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(w, X, y) + (prior_prec / 2) * torch.sum(w**2)\n",
    "\n",
    "        # compute the gradient of loss w.r.t. w.\n",
    "        loss.backward()\n",
    "        # Put the gradient and loss back into Numpy.\n",
    "        grad = w.grad.detach().numpy()\n",
    "        loss = loss.item()\n",
    "\n",
    "        return loss, grad\n",
    "\n",
    "    return closure\n",
    "\n",
    "# PyTorch Loss Functions\n",
    "\n",
    "def logistic_loss(w, X, y):\n",
    "    ''' Logistic Loss'''\n",
    "    n,d = X.shape\n",
    "    return torch.mean(torch.log(1 + torch.exp(-torch.mul(y, torch.matmul(X, w)))))\n",
    "\n",
    "def squared_hinge_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Hinge Loss '''\n",
    "    return torch.mean((torch.max( torch.zeros(n,dtype=torch.double) , torch.ones(n,dtype=torch.double) - torch.mul(y, torch.matmul(X, w))))**2 )\n",
    "\n",
    "def squared_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Loss'''\n",
    "    return torch.mean(( y - torch.matmul(X, w) )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBSVM_URL = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/\"\n",
    "LIBSVM_DOWNLOAD_FN = {\"rcv1\"       : \"rcv1_train.binary.bz2\",\n",
    "                      \"mushrooms\"  : \"mushrooms\",\n",
    "                      \"a1a\"  : \"a1a\",\n",
    "                      \"a2a\"  : \"a2a\",\n",
    "                      \"ijcnn\"      : \"ijcnn1.tr.bz2\",\n",
    "                      \"w8a\"        : \"w8a\"}\n",
    "\n",
    "\n",
    "\n",
    "def load_libsvm(name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    fn = LIBSVM_DOWNLOAD_FN[name]\n",
    "    data_path = os.path.join(data_dir, fn)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        url = urllib.parse.urljoin(LIBSVM_URL, fn)\n",
    "        print(\"Downloading from %s\" % url)\n",
    "        urllib.request.urlretrieve(url, data_path)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    X, y = load_svmlight_file(data_path)\n",
    "    return [X, y]\n",
    "\n",
    "def data_load(data_dir, dataset_num, n = 0, d = 0, margin = 1e-6, false_ratio = 0, is_subsample = 0, is_kernelize = 0, test_prop=0.2, split_seed=9513451):\n",
    "\n",
    "    if dataset_num == 0:\n",
    "        data_name = 'quantum'\n",
    "\n",
    "    elif dataset_num == 1:\n",
    "        data_name = 'rcv1'\n",
    "\n",
    "    elif dataset_num == 2:\n",
    "        data_name = 'protein'\n",
    "\n",
    "    elif dataset_num == 3:\n",
    "        data_name = 'news'\n",
    "\n",
    "    elif dataset_num == 4:\n",
    "        data_name = 'mushrooms'\n",
    "\n",
    "    elif dataset_num == 5:\n",
    "        data_name = 'splice'\n",
    "\n",
    "    elif dataset_num == 6:\n",
    "        data_name = 'ijcnn'\n",
    "\n",
    "    elif dataset_num == 7:\n",
    "        data_name = 'w8a'\n",
    "\n",
    "    elif dataset_num == 8:\n",
    "        data_name = 'covtype'\n",
    "\n",
    "    elif dataset_num == -1:\n",
    "        data_name = 'synthetic'\n",
    "\n",
    "    if (dataset_num >= 0):\n",
    "\n",
    "        # real data\n",
    "#         data = pickle.load(open(data_dir + data_name +'.pkl', 'rb'), encoding = \"latin1\")\n",
    "        data = load_libsvm(data_name, data_dir='./')\n",
    "\n",
    "        # load real dataset\n",
    "        A = data[0].toarray()\n",
    "\n",
    "        if dataset_num < 4:\n",
    "            y = data[1].toarray().ravel()\n",
    "        else:\n",
    "            y = data[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        A, y, w_true = create_dataset(n,d, margin, false_ratio)\n",
    "\n",
    "        # generate synthetic data - according to the BB paper\n",
    "#         x = np.random.randn(n, d)\n",
    "#         w_true = np.random.randn(d)\n",
    "#         y = np.sign(np.dot(x, w_true))\n",
    "\n",
    "    # subsample\n",
    "    if is_subsample == 1:\n",
    "        A = A[:n,:]\n",
    "        y = y[:n]\n",
    "\n",
    "    # split dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(A, y, test_size=test_prop, random_state=split_seed)\n",
    "\n",
    "    if is_kernelize == 1:\n",
    "        # Form kernel\n",
    "        A_train, A_test = kernelize(X_train, X_test, dataset_num, data_dir=data_dir)\n",
    "    else:\n",
    "        A_train = X_train\n",
    "        A_test = X_test\n",
    "\n",
    "    print('Loaded ', data_name ,' dataset.')\n",
    "\n",
    "    return A_train, y_train, A_test, y_test\n",
    "\n",
    "def kernelize(X, X_test, dataset_num, kernel_type=0, data_dir=\"./Data\"):\n",
    "\n",
    "    n = X.shape[0]\n",
    "\n",
    "    fname = data_dir + '/Kernel_' + str(n) + '_' + str(dataset_num) + '.p'\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "\n",
    "        print('Reading file ', fname)\n",
    "        X_kernel, X_test_kernel = pickle.load( open( fname, \"rb\" ) )\n",
    "\n",
    "    else:\n",
    "        if kernel_type == 0:\n",
    "            X_kernel = RBF_kernel(X, X)\n",
    "            X_test_kernel = RBF_kernel(X_test, X)\n",
    "            print('Formed the kernel matrix')\n",
    "\n",
    "        pickle.dump( (X_kernel, X_test_kernel) , open( fname, \"wb\" ) )\n",
    "\n",
    "    return X_kernel, X_test_kernel\n",
    "\n",
    "def RBF_kernel( A, B, sigma = 1.0 ):\n",
    "\n",
    "    distance_2 = np.square(  metrics.pairwise.pairwise_distances( X = A, Y = B, metric='euclidean'  )   )\n",
    "    K = np.exp( -1 * np.divide( distance_2, (2 * (sigma**2)) )  )\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def create_dataset(n,d,gamma = 0, false_ratio = 0):\n",
    "# create synthetic dataset using the python utility\n",
    "# X, y = datasets.make_classification(n_samples=n, n_features=d,n_informative = d, n_redundant = 0, class_sep = 2.0 )\n",
    "# convert into -1/+1\n",
    "# y = 2 * y - 1\n",
    "\n",
    "# create linearly separable dataset with margin gamma\n",
    "#w_star = np.random.random((d,1))\n",
    "    w_star = np.random.normal(0,1,(d,1))\n",
    "# normalize w_star\n",
    "    w_star = w_star / np.linalg.norm(w_star)\n",
    "\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    count = 0\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    y = np.zeros((n))\n",
    "\n",
    "    while(1):\n",
    "\n",
    "        x = np.random.normal( 1,1,(1,d) ) \n",
    "        # normalize x s.t. || x ||_2 = 1\n",
    "#         x = x / np.linalg.norm(x)\n",
    "\n",
    "        temp = np.dot( x, w_star )\n",
    "        margin = abs( temp )\n",
    "        sig = np.sign( temp )\n",
    "\n",
    "        if margin > gamma * np.linalg.norm(w_star):\n",
    "\n",
    "            if count % 2 == 0:\n",
    "\n",
    "                # generate positive\n",
    "                if sig > 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count, :] = -x\n",
    "                y[ count ] = + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                # generate negative\n",
    "                if sig < 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count,:] = -x\n",
    "                y[ count ] = - 1\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        if count == n:\n",
    "            break\n",
    "            \n",
    "    flip_ind = np.random.choice(n, int(n*false_ratio))\n",
    "    y[flip_ind] = -y[flip_ind]\n",
    "    \n",
    "    return X, y, w_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_minibatches(n, m, minibatch_size):\n",
    "    ''' Create m minibatches from the training set by sampling without replacement.\n",
    "        This function may sample the training set multiple times.\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        m: number of minibatches to generate\n",
    "        batch_size: size of the desired minibatches'''\n",
    "\n",
    "    k = math.ceil(m * minibatch_size / n)\n",
    "    batches = []\n",
    "    for i in range(k):\n",
    "        batches += minibatch_data(n, minibatch_size)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def minibatch_data(n, batch_size):\n",
    "    '''Splits training set into minibatches by sampling **without** replacement.\n",
    "    This isn't performant for large datasets (e.g. we should switch to PyTorch's streaming data loader eventually).\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        batch_size: size of the desired minibatches'''\n",
    "    # shuffle training set indices before forming minibatches\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    batches = []\n",
    "    num_batches = math.floor(n / batch_size)\n",
    "    # split the training set into minibatches\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        stop_index = (batch_num + 1) * batch_size\n",
    "\n",
    "        # create a minibatch\n",
    "        batches.append(indices[start_index:stop_index])\n",
    "\n",
    "    # generate a final, smaller batch if the batch_size doesn't divide evenly into n\n",
    "    if num_batches != math.ceil(n / batch_size):\n",
    "        batches.append(indices[stop_index:])\n",
    "\n",
    "    return batches\n",
    "\n",
    "def reset(model):\n",
    "    # reset the model\n",
    "    for param in model.parameters():\n",
    "        param.data = torch.zeros_like(param)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plotting(results, labels, max_epochs):\n",
    "    plt.figure()\n",
    "\n",
    "    offset = 0\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot( x, ( results[i,:] ), color = colors[i], label = labels[i] )\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_shaded_error_bars(results_mean, results_std, max_epochs, ylabel=\"Loss\", colors=None, labels=None):\n",
    "    fig = plt.figure()\n",
    "    offset = 0\n",
    "    if colors is None:\n",
    "        colors = ['r', 'b', 'g','k','cyan','lightgreen']\n",
    "    if labels is None:\n",
    "        labels = ['SVRG-BB', 'SVRG']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(results_mean.shape[0]):\n",
    "        plt.plot( x, ( results_mean[i,:] ), color = colors[i], label = labels[i] )\n",
    "        plt.fill_between(x, (results_mean[i,:] - results_std[i,:]), (results_mean[i,:] + results_std[i,:]), color=colors[i], alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    step_size = init_step_size\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            output += ', Num gradient evaluations: %d' % num_grad_evals\n",
    "            print(output)\n",
    "\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "        \n",
    "        \n",
    "        num_grad_evals = num_grad_evals + n\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "            \n",
    "            num_grad_evals = num_grad_evals + 1\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None, \n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size in the inner loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    AVERAGE_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "    MAX_ARMIJO_STEPS_REACHED = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, max_step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            f_x ,x_grad = closure(x, Di, labels_i)\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "            g = x_grad - x_tilde_grad + full_grad\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            \n",
    "            step_size = start_step_size\n",
    "            found = False\n",
    "            for j in range(max_iter_armijo):\n",
    "                if closure(x - step_size*g, Di, labels_i)[0] > f_x - c*step_size*g_norm:\n",
    "                    step_size *= beta\n",
    "                else:\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            AVERAGE_ARMIJO_STEPS[k] += j\n",
    "            if found:\n",
    "                x -= step_size * g\n",
    "                if not reset_step_size:\n",
    "                    start_step_size = step_size\n",
    "            else:\n",
    "                MAX_ARMIJO_STEPS_REACHED[k]+=1\n",
    "                x -= 1e-6*g\n",
    "        AVERAGE_ARMIJO_STEPS[k] /= m\n",
    "        output = 'Epoch.: %d, Avg armijo steps: %.2e, Nb max iteration reached: %.2e' % \\\n",
    "                     (k, AVERAGE_ARMIJO_STEPS[k], MAX_ARMIJO_STEPS_REACHED[k])\n",
    "        print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo_outer_end(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size at the end of the outer loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    \n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    NB_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        \n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        norm_full_grad = np.linalg.norm(full_grad)\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                    (k, start_step_size, norm_full_grad)\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "        \n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = norm_full_grad\n",
    "        step_size = start_step_size    \n",
    "        \n",
    "        for j in range(max_iter_armijo):\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                f_x ,x_grad = closure(x, Di, labels_i)\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "                g = x_grad - x_tilde_grad + full_grad\n",
    "                x -= step_size * g\n",
    "                \n",
    "            if closure(x, D, labels)[0] > loss - c*step_size*norm_full_grad:\n",
    "                step_size = beta*step_size\n",
    "                x = x_tilde.copy()\n",
    "                NB_ARMIJO_STEPS[k]+=1\n",
    "            else:\n",
    "                found = True\n",
    "                break\n",
    "        AVERAGE_ARMIJO_STEPS[k] += j\n",
    "        if found:\n",
    "            if not reset_step_size:\n",
    "                    start_step_size = step_size\n",
    "            x -= step_size * g\n",
    "        if not found:\n",
    "            x -= 1e-6*full_grad\n",
    "            \n",
    "        output = 'Epoch.: %d, Nb of Armijo steps: %.2e' % \\\n",
    "                         (k, NB_ARMIJO_STEPS[k])\n",
    "        print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True, adaptive_termination = False):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        \n",
    "#         if k == 0:\n",
    "        \n",
    "#             Gk2 = 0\n",
    "#             step_size = init_step_size\n",
    "                        \n",
    "#             minibatches = make_minibatches(n, m, batch_size)\n",
    "#             for i in range(m):\n",
    "#                 # get the minibatch for this iteration\n",
    "#                 indices = minibatches[i]\n",
    "#                 Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "#                 # compute the gradients:\n",
    "#                 x_grad = closure(x, Di, labels_i)[1]\n",
    "\n",
    "#                 gk = x_grad \n",
    "\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "\n",
    "#                 x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "#                 num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "#                 if adaptive_termination == True:\n",
    "\n",
    "#                     if i == 0:\n",
    "#                         org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                     else:\n",
    "#                         temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                         # check condition to terminate inner loop\n",
    "#                         if temp/org < 1e-2:\n",
    "#                             print('Breaking from inner loop')                    \n",
    "#                             break                            \n",
    "        \n",
    "        if k >= 0:\n",
    "            print('Here')\n",
    "            loss, full_grad = closure(x, D, labels)\n",
    "            x_tilde = x.copy()\n",
    "\n",
    "            last_full_grad = full_grad\n",
    "            last_x_tilde = x_tilde\n",
    "\n",
    "            # initialize running sum of gradient norms\n",
    "            Gk2 = 0\n",
    "            step_size = init_step_size\n",
    "\n",
    "            if verbose:\n",
    "                output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                         (k, np.linalg.norm(full_grad))\n",
    "                output += ', Func. value: %e' % loss\n",
    "                output += ', Num gradient evaluations: %d' % num_grad_evals            \n",
    "                print(output)\n",
    "\n",
    "            if np.linalg.norm(full_grad) <= 1e-14:\n",
    "                return x, LOSS, GRAD_NORM\n",
    "\n",
    "            LOSS[k] = loss\n",
    "            GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "            num_grad_evals =  num_grad_evals  + n\n",
    "\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                x_grad = closure(x, Di, labels_i)[1]\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "                gk = x_grad - x_tilde_grad + full_grad\n",
    "\n",
    "                Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(x_grad) ** 2)\n",
    "\n",
    "                x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "                num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "                if adaptive_termination == True:\n",
    "\n",
    "                    if i == 0:\n",
    "                        org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                    else:\n",
    "                        temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                        # check condition to terminate inner loop\n",
    "                        if temp/org < 1e-2 and i >=  int(n / batch_size):\n",
    "                            print(temp/org)\n",
    "                            print('Breaking from inner loop')                                \n",
    "                            break\n",
    "            \n",
    "            \n",
    "    return x, LOSS, GRAD_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        \n",
    "        Gk2 = 0\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            gk = x_grad - x_tilde_grad + full_grad\n",
    "            \n",
    "            Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "                        \n",
    "            x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "dataset_num = 4\n",
    "data_dir = './'\n",
    "\n",
    "is_subsample = 0\n",
    "is_kernelize = 0\n",
    "subsampled_n = -1\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(6162647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization options\n",
    "max_epochs = 10\n",
    "num_restarts = 1\n",
    "batch_size = 10\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded  mushrooms  dataset.\n"
     ]
    }
   ],
   "source": [
    "# problem size when generating synthetic data\n",
    "if dataset_num == -1:\n",
    "    n, d = 10000, 20\n",
    "    false_ratio = 0.25\n",
    "    margin = 1e-6\n",
    "    print(is_kernelize)\n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num,n, d, margin, false_ratio)\n",
    "else:\n",
    "    if is_subsample == 1:\n",
    "        n = subsampled_n\n",
    "    else:\n",
    "        n = 0\n",
    "    if is_kernelize == 1:\n",
    "        d = n\n",
    "    else:\n",
    "        d = 0\n",
    "        \n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num, n,d, false_ratio, is_subsample, is_kernelize)\n",
    "    \n",
    "    if n == 0:\n",
    "        n = A.shape[0]\n",
    "    \n",
    "    \n",
    "#define the regularized losses we will use\n",
    "logistic_closure_l2 = make_closure(logistic_loss, 1/n)\n",
    "squared_hinge_closure_l2 = make_closure(squared_hinge_loss, 1/n)\n",
    "squared_closure_l2 = make_closure(squared_loss, 1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment\n",
    "exp_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if exp_num == 0:\n",
    "    print('Testing SVRG-BB:')\n",
    "    init_eta = 1e-3\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] = run_experiment(svrg_bb, logistic_closure_no_l2, A, y, A_test, y_test, init_eta, batch_size, max_epochs, num_restarts, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 1:\n",
    "    print('Testing robustness to step-size for SVRG:')    \n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = 'svrg', closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 2:\n",
    "    print('Testing robustness to size of inner loop for SVRG:')   \n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    init_eta = 1e-2\n",
    "    inner_loop_list = [0.5 , 1, 2, 5, 10]\n",
    "\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(inner_loop_list)\n",
    "\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for inner_loop in inner_loop_list: \n",
    "        print('Begin to run SVRG with inner loop size = :', inner_loop)\n",
    "\n",
    "        m = int(inner_loop * n)\n",
    "\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = m, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(inner_loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 3:\n",
    "    \n",
    "    labels = []\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    step_sizes = [1e-4, 1e-2, 1, 100]\n",
    "        \n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))     \n",
    "        \n",
    "    for init_eta in step_sizes:     \n",
    "  \n",
    "        print('Testing SVRG-Ada with step-size:', init_eta)\n",
    "    \n",
    "        x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = squared_closure_l2, \\\n",
    "                           X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                           init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                           m = 0, num_restarts = num_restarts, verbose=verbose) \n",
    "    \n",
    "        labels.append(['SVRG-Ada-', str(init_eta)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVRG vs SVRG-Ada vs SVRG-BB wrt step-size robustness:\n",
      "Begin to run SVRG with step-size = : 0.001\n",
      "Running Experiment:\n",
      "Epoch.: 0, Grad. norm: 9.82e+00, Func. value: 2.541314e+00, Num gradient evaluations: 0\n",
      "Epoch.: 1, Grad. norm: 6.56e-02, Func. value: 2.798640e-02, Num gradient evaluations: 9744\n",
      "Epoch.: 2, Grad. norm: 3.89e-02, Func. value: 2.034339e-02, Num gradient evaluations: 19488\n",
      "Epoch.: 3, Grad. norm: 3.08e-02, Func. value: 1.650088e-02, Num gradient evaluations: 29232\n",
      "Epoch.: 4, Grad. norm: 2.64e-02, Func. value: 1.388928e-02, Num gradient evaluations: 38976\n",
      "Epoch.: 5, Grad. norm: 2.31e-02, Func. value: 1.193126e-02, Num gradient evaluations: 48720\n",
      "Epoch.: 6, Grad. norm: 2.06e-02, Func. value: 1.039673e-02, Num gradient evaluations: 58464\n",
      "Epoch.: 7, Grad. norm: 1.86e-02, Func. value: 9.164710e-03, Num gradient evaluations: 68208\n",
      "Epoch.: 8, Grad. norm: 1.73e-02, Func. value: 8.159038e-03, Num gradient evaluations: 77952\n",
      "Epoch.: 9, Grad. norm: 1.53e-02, Func. value: 7.326770e-03, Num gradient evaluations: 87696\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Running Experiment:\n",
      "Epoch.: 0, Step size: 1.00e-03, Grad. norm: 9.82e+00, Func. value: 2.541314e+00\n",
      "Epoch.: 1, Step size: 3.01e-05, Grad. norm: 6.56e-02, Func. value: 2.798640e-02\n",
      "Epoch.: 2, Step size: 6.26e-04, Grad. norm: 6.22e-02, Func. value: 2.759199e-02\n",
      "Epoch.: 3, Step size: 1.78e-03, Grad. norm: 4.38e-02, Func. value: 2.222836e-02\n",
      "Epoch.: 4, Step size: 4.22e-03, Grad. norm: 2.95e-02, Func. value: 1.525640e-02\n",
      "Epoch.: 5, Step size: 8.42e-03, Grad. norm: 1.85e-02, Func. value: 8.494833e-03\n",
      "Epoch.: 6, Step size: 1.21e-02, Grad. norm: 2.61e-02, Func. value: 4.171769e-03\n",
      "Epoch.: 7, Step size: 1.62e-02, Grad. norm: 1.45e-02, Func. value: 2.543040e-03\n",
      "Epoch.: 8, Step size: 2.53e-02, Grad. norm: 3.90e-03, Func. value: 1.950207e-03\n",
      "Epoch.: 9, Step size: 4.10e-02, Grad. norm: 4.70e-03, Func. value: 1.666873e-03\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Running Experiment:\n",
      "Here\n",
      "Epoch.: 0, Grad. norm: 9.82e+00, Func. value: 2.541314e+00, Num gradient evaluations: 0\n",
      "Here\n",
      "Epoch.: 1, Grad. norm: 7.62e+00, Func. value: 1.603630e+00, Num gradient evaluations: 9744\n",
      "Here\n",
      "Epoch.: 2, Grad. norm: 5.46e+00, Func. value: 9.103716e-01, Num gradient evaluations: 19488\n",
      "Here\n",
      "Epoch.: 3, Grad. norm: 3.40e+00, Func. value: 4.532384e-01, Num gradient evaluations: 29232\n",
      "Here\n",
      "Epoch.: 4, Grad. norm: 1.59e+00, Func. value: 2.121610e-01, Num gradient evaluations: 38976\n",
      "Here\n",
      "Epoch.: 5, Grad. norm: 5.85e-01, Func. value: 1.232054e-01, Num gradient evaluations: 48720\n",
      "Here\n",
      "Epoch.: 6, Grad. norm: 3.25e-01, Func. value: 8.062001e-02, Num gradient evaluations: 58464\n",
      "Here\n",
      "Epoch.: 7, Grad. norm: 1.92e-01, Func. value: 5.680339e-02, Num gradient evaluations: 68208\n",
      "Here\n",
      "Epoch.: 8, Grad. norm: 1.25e-01, Func. value: 4.192474e-02, Num gradient evaluations: 77952\n",
      "Here\n",
      "Epoch.: 9, Grad. norm: 8.21e-02, Func. value: 3.253393e-02, Num gradient evaluations: 87696\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Begin to run SVRG with step-size = : 0.01\n",
      "Running Experiment:\n",
      "Epoch.: 0, Grad. norm: 9.82e+00, Func. value: 2.541314e+00, Num gradient evaluations: 0\n",
      "Epoch.: 1, Grad. norm: 1.68e-01, Func. value: 1.736642e-02, Num gradient evaluations: 9744\n",
      "Epoch.: 2, Grad. norm: 1.03e-01, Func. value: 3.700464e-03, Num gradient evaluations: 19488\n",
      "Epoch.: 3, Grad. norm: 6.18e-03, Func. value: 2.429235e-03, Num gradient evaluations: 29232\n",
      "Epoch.: 4, Grad. norm: 3.85e-03, Func. value: 2.043276e-03, Num gradient evaluations: 38976\n",
      "Epoch.: 5, Grad. norm: 2.60e-03, Func. value: 1.848928e-03, Num gradient evaluations: 48720\n",
      "Epoch.: 6, Grad. norm: 1.85e-03, Func. value: 1.735077e-03, Num gradient evaluations: 58464\n",
      "Epoch.: 7, Grad. norm: 3.93e-03, Func. value: 1.661905e-03, Num gradient evaluations: 68208\n",
      "Epoch.: 8, Grad. norm: 1.72e-03, Func. value: 1.610763e-03, Num gradient evaluations: 77952\n",
      "Epoch.: 9, Grad. norm: 1.01e-03, Func. value: 1.573631e-03, Num gradient evaluations: 87696\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Running Experiment:\n",
      "Epoch.: 0, Step size: 1.00e-02, Grad. norm: 9.82e+00, Func. value: 2.541314e+00\n",
      "Epoch.: 1, Step size: 7.43e-05, Grad. norm: 1.68e-01, Func. value: 1.736642e-02\n",
      "Epoch.: 2, Step size: 1.89e-04, Grad. norm: 1.09e-01, Func. value: 1.318504e-02\n",
      "Epoch.: 3, Step size: 3.10e-04, Grad. norm: 5.99e-02, Func. value: 9.113238e-03\n",
      "Epoch.: 4, Step size: 4.75e-04, Grad. norm: 3.15e-02, Func. value: 7.172482e-03\n",
      "Epoch.: 5, Step size: 8.32e-04, Grad. norm: 1.81e-02, Func. value: 6.310742e-03\n",
      "Epoch.: 6, Step size: 2.60e-03, Grad. norm: 1.31e-02, Func. value: 5.704952e-03\n",
      "Epoch.: 7, Step size: 1.02e-02, Grad. norm: 1.01e-02, Func. value: 4.631958e-03\n",
      "Epoch.: 8, Step size: 1.57e-02, Grad. norm: 1.66e-02, Func. value: 2.819952e-03\n",
      "Epoch.: 9, Step size: 2.28e-02, Grad. norm: 1.17e-02, Func. value: 2.036805e-03\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Running Experiment:\n",
      "Here\n",
      "Epoch.: 0, Grad. norm: 9.82e+00, Func. value: 2.541314e+00, Num gradient evaluations: 0\n",
      "Here\n",
      "Epoch.: 1, Grad. norm: 5.18e-01, Func. value: 1.133480e-01, Num gradient evaluations: 9744\n",
      "Here\n",
      "Epoch.: 2, Grad. norm: 8.40e-02, Func. value: 3.235930e-02, Num gradient evaluations: 19488\n",
      "0.009990456411241853\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 3, Grad. norm: 4.51e-02, Func. value: 1.792517e-02, Num gradient evaluations: 27843\n",
      "0.00999384921277658\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 4, Grad. norm: 2.55e-02, Func. value: 1.020101e-02, Num gradient evaluations: 36072\n",
      "0.009998321839450265\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 5, Grad. norm: 1.44e-02, Func. value: 6.196382e-03, Num gradient evaluations: 43893\n",
      "0.009997784137813055\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 6, Grad. norm: 1.76e-02, Func. value: 4.168608e-03, Num gradient evaluations: 51314\n",
      "0.009994636379986384\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 7, Grad. norm: 1.20e-02, Func. value: 2.855960e-03, Num gradient evaluations: 59616\n",
      "0.00998465585511514\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 8, Grad. norm: 1.13e-02, Func. value: 2.212032e-03, Num gradient evaluations: 67813\n",
      "0.00999131644577465\n",
      "Breaking from inner loop\n",
      "Here\n",
      "Epoch.: 9, Grad. norm: 6.01e-03, Func. value: 1.840369e-03, Num gradient evaluations: 76833\n",
      "0.009977022100446382\n",
      "Breaking from inner loop\n",
      "Restart 0, Test accuracy: 0.465231\n",
      "Begin to run SVRG with step-size = : 0.1\n",
      "Running Experiment:\n",
      "Epoch.: 0, Grad. norm: 9.82e+00, Func. value: 2.541314e+00, Num gradient evaluations: 0\n",
      "Epoch.: 1, Grad. norm: 4.45e+72, Func. value: 4.918930e+143, Num gradient evaluations: 9744\n",
      "Epoch.: 2, Grad. norm: 2.18e+139, Func. value: 1.211366e+277, Num gradient evaluations: 19488\n",
      "Epoch.: 3, Grad. norm: inf, Func. value: inf, Num gradient evaluations: 29232\n",
      "Epoch.: 4, Grad. norm: inf, Func. value: inf, Num gradient evaluations: 38976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch.: 5, Grad. norm: nan, Func. value: nan, Num gradient evaluations: 48720\n",
      "Epoch.: 6, Grad. norm: nan, Func. value: nan, Num gradient evaluations: 58464\n",
      "Epoch.: 7, Grad. norm: nan, Func. value: nan, Num gradient evaluations: 68208\n"
     ]
    }
   ],
   "source": [
    "if exp_num == 4:\n",
    "    print('Testing SVRG vs SVRG-Ada vs SVRG-BB wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-3, 1e-2, 1e-1]\n",
    "    colors = ['r', 'b', 'g','r','b','g','r', 'b', 'g']\n",
    "\n",
    "    num_variants = 3 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = squared_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "#         # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "        \n",
    "        \n",
    "        # SVRG BB\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('BB' + str(init_eta))\n",
    "        \n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose,  adaptive_termination = True)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 5:\n",
    "    print('Testing SVRG-BB vs SVRG-BB-Ada wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-2, 1e-1, 1]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = 2 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = logistic_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "        # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 6:\n",
    "    print('Testing SVRG-Armijo:')\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "\n",
    "    x_mean, results_mean2[0, :], results_std2[0, :], results2_mean2[0, :], results2_std2[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_armijo\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = n, num_restarts = num_restarts, verbose=True, init_step_size = 0.01, c=0.1, max_step_size = 0.1, \\\n",
    "                       beta = 0.7, max_iter_armijo=10, reset_step_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g','r','b','g','r', 'b', 'g']\n",
    "fig = plot_shaded_error_bars(results_mean = results_mean, results_std = results_std, ylabel = 'Loss', max_epochs = max_epochs, colors = colors, labels = labels)\n",
    "plt.show()\n",
    "\n",
    "fig = plot_shaded_error_bars(results_mean = results2_mean, results_std = results2_std, ylabel = 'Grad_norm', max_epochs = max_epochs, colors = colors, labels = labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
