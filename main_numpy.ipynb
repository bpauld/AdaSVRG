{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import datasets, metrics\n",
    "import urllib\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(method_name, \n",
    "                   closure, \n",
    "                   X, \n",
    "                   y, \n",
    "                   X_test, \n",
    "                   y_test,\n",
    "                   batch_size=10, \n",
    "                   max_epochs=50, \n",
    "                   m = 0, \n",
    "                   num_restarts=10, \n",
    "                   verbose=False, \n",
    "                   seed=9513451,\n",
    "                   **kwargs):\n",
    "    '''Run an experiment with multiple restarts and compute basic statistics from the runs.'''\n",
    "    # set the experiment seed\n",
    "    print(\"Running Experiment:\")\n",
    "    np.random.seed(seed)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "    line_search_results = []\n",
    "    arg_dict = kwargs\n",
    "\n",
    "    n,d = X.shape\n",
    "    n_test = X_test.shape[0]\n",
    "    x_sum = np.zeros(d)\n",
    "\n",
    "    # do the restarts\n",
    "    for i in range(num_restarts):\n",
    "        if method_name==\"svrg\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_bb\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo(closure = closure, \n",
    "                                                                               batch_size = batch_size,\n",
    "                                                                               D = X,\n",
    "                                                                               labels = y,\n",
    "                                                                               n = n,\n",
    "                                                                               d = d,\n",
    "                                                                               c=c,\n",
    "                                                                               beta=beta,\n",
    "                                                                               max_iter_armijo=max_iter_armijo,\n",
    "                                                                               max_step_size=max_step_size,\n",
    "                                                                               reset_step_size=reset_step_size,\n",
    "                                                                               max_epoch=max_epochs,\n",
    "                                                                               m = m,\n",
    "                                                                               verbose=verbose)\n",
    "        elif method_name==\"svrg_armijo_outer_end\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo_outer_end(closure = closure, \n",
    "                                                                                         batch_size = batch_size,\n",
    "                                                                                         D = X,\n",
    "                                                                                         labels = y,\n",
    "                                                                                         n = n,\n",
    "                                                                                         d = d,\n",
    "                                                                                         c=c,\n",
    "                                                                                         beta=beta,\n",
    "                                                                                         max_iter_armijo=max_iter_armijo,\n",
    "                                                                                         max_step_size=max_step_size,\n",
    "                                                                                         reset_step_size=reset_step_size,\n",
    "                                                                                         max_epoch=max_epochs,\n",
    "                                                                                         m = m,\n",
    "                                                                                         verbose=verbose)\n",
    "        elif method_name == \"svrg_armijo_outer_beginning\":\n",
    "            c = arg_dict[\"c\"]\n",
    "            beta = arg_dict[\"beta\"]\n",
    "            max_iter_armijo = arg_dict[\"max_iter_armijo\"]\n",
    "            max_step_size = arg_dict[\"max_step_size\"]\n",
    "            reset_step_size = arg_dict[\"reset_step_size\"]\n",
    "            x, loss_record, gradnorm_record, line_search_records = svrg_armijo_outer_beginning(closure = closure, \n",
    "                                                                                               batch_size = batch_size,\n",
    "                                                                                               D = X,\n",
    "                                                                                               labels = y,\n",
    "                                                                                               n = n,\n",
    "                                                                                               d = d,\n",
    "                                                                                               c=c,\n",
    "                                                                                               beta=beta,\n",
    "                                                                                               max_iter_armijo=max_iter_armijo,\n",
    "                                                                                               max_step_size=max_step_size,\n",
    "                                                                                               reset_step_size=reset_step_size,\n",
    "                                                                                               max_epoch=max_epochs,\n",
    "                                                                                               m = m,\n",
    "                                                                                               verbose=verbose)\n",
    "        elif method_name == \"svrg_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            adaptive_termination = arg_dict[\"adaptive_termination\"]            \n",
    "            x, loss_record, gradnorm_record = svrg_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose, adaptive_termination = adaptive_termination)   \n",
    "            \n",
    "        elif method_name == \"svrg_bb_ada\":\n",
    "            init_step_size = arg_dict[\"init_step_size\"]\n",
    "            x, loss_record, gradnorm_record = svrg_bb_ada(closure = closure, \n",
    "                                                   batch_size = batch_size,\n",
    "                                                   D = X,\n",
    "                                                   labels = y,\n",
    "                                                   init_step_size = init_step_size,\n",
    "                                                   n = n,\n",
    "                                                   d = d,\n",
    "                                                   max_epoch=max_epochs,\n",
    "                                                   m = m,\n",
    "                                                   verbose=verbose)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Method does not exist')\n",
    "            \n",
    "        x_sum += x\n",
    "        loss_results.append(loss_record)\n",
    "        gradnorm_results.append(gradnorm_record)\n",
    "\n",
    "        if verbose:\n",
    "            y_predict = np.sign(np.dot(X_test, x))\n",
    "            print('Restart %d, Test accuracy: %f' % (i, (np.count_nonzero(y_test == y_predict)*1.0 / n_test)))\n",
    "\n",
    "    # compute basic statistics from the runs\n",
    "    x_mean = x_sum / num_restarts\n",
    "\n",
    "    loss_results = np.stack(loss_results)\n",
    "    loss_std = loss_results.std(axis=0)\n",
    "    loss_mean = loss_results.mean(axis=0)\n",
    "\n",
    "    gradnorm_results = np.stack(gradnorm_results)\n",
    "    gradnorm_std = gradnorm_results.std(axis=0)\n",
    "    gradnorm_mean = gradnorm_results.mean(axis=0)\n",
    "\n",
    "    if method_name in [\"svrg_armijo\", \"svrg_armijo_outer_end\", \"svrg_armijo_outer_beginning\"]:\n",
    "        line_search_results.append(line_search_records)\n",
    "        line_search_results = np.stack(line_search_results)\n",
    "        line_search_std = line_search_results.std(axis=0)\n",
    "        line_search_mean = line_search_results.mean(axis=0)\n",
    "        return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std, line_search_mean, line_search_std\n",
    "    else:                           \n",
    "        return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closure(loss_fn, prior_prec=1e-2):\n",
    "    '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            loss_fn: the loss function to use (logistic loss, hinge loss, squared error, etc)\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: a closure fn for computing the loss and gradient. '''\n",
    "\n",
    "    def closure(w, X, y):\n",
    "        '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            w: weight vector\n",
    "            X: minibatch of input vectors\n",
    "            y: labels for the input vectors\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: (loss, gradient)'''\n",
    "        # change the Numpy Arrays into PyTorch Tensors\n",
    "        X = torch.tensor(X)\n",
    "        # Type of X is double, so y must be double.\n",
    "        y = torch.tensor(y, dtype=torch.double)\n",
    "        w = torch.tensor(w, requires_grad=True)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(w, X, y) + (prior_prec / 2) * torch.sum(w**2)\n",
    "\n",
    "        # compute the gradient of loss w.r.t. w.\n",
    "        loss.backward()\n",
    "        # Put the gradient and loss back into Numpy.\n",
    "        grad = w.grad.detach().numpy()\n",
    "        loss = loss.item()\n",
    "\n",
    "        return loss, grad\n",
    "\n",
    "    return closure\n",
    "\n",
    "# PyTorch Loss Functions\n",
    "\n",
    "def logistic_loss(w, X, y):\n",
    "    ''' Logistic Loss'''\n",
    "    n,d = X.shape\n",
    "    return torch.mean(torch.log(1 + torch.exp(-torch.mul(y, torch.matmul(X, w)))))\n",
    "\n",
    "def squared_hinge_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Hinge Loss '''\n",
    "    return torch.mean((torch.max( torch.zeros(n,dtype=torch.double) , torch.ones(n,dtype=torch.double) - torch.mul(y, torch.matmul(X, w))))**2 )\n",
    "\n",
    "def squared_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Loss'''\n",
    "    return torch.mean(( y - torch.matmul(X, w) )**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIBSVM_URL = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/\"\n",
    "LIBSVM_DOWNLOAD_FN = {\"rcv1\"       : \"rcv1_train.binary.bz2\",\n",
    "                      \"mushrooms\"  : \"mushrooms\",\n",
    "                      \"a1a\"  : \"a1a\",\n",
    "                      \"a2a\"  : \"a2a\",\n",
    "                      \"ijcnn\"      : \"ijcnn1.tr.bz2\",\n",
    "                      \"w8a\"        : \"w8a\"}\n",
    "\n",
    "\n",
    "\n",
    "def load_libsvm(name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    fn = LIBSVM_DOWNLOAD_FN[name]\n",
    "    data_path = os.path.join(data_dir, fn)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        url = urllib.parse.urljoin(LIBSVM_URL, fn)\n",
    "        print(\"Downloading from %s\" % url)\n",
    "        urllib.request.urlretrieve(url, data_path)\n",
    "        print(\"Download complete.\")\n",
    "\n",
    "    X, y = load_svmlight_file(data_path)\n",
    "    return [X, y]\n",
    "\n",
    "def data_load(data_dir, dataset_num, n = 0, d = 0, margin = 1e-6, false_ratio = 0, is_subsample = 0, is_kernelize = 0, test_prop=0.2, split_seed=9513451):\n",
    "\n",
    "    if dataset_num == 0:\n",
    "        data_name = 'quantum'\n",
    "\n",
    "    elif dataset_num == 1:\n",
    "        data_name = 'rcv1'\n",
    "\n",
    "    elif dataset_num == 2:\n",
    "        data_name = 'protein'\n",
    "\n",
    "    elif dataset_num == 3:\n",
    "        data_name = 'news'\n",
    "\n",
    "    elif dataset_num == 4:\n",
    "        data_name = 'mushrooms'\n",
    "\n",
    "    elif dataset_num == 5:\n",
    "        data_name = 'splice'\n",
    "\n",
    "    elif dataset_num == 6:\n",
    "        data_name = 'ijcnn'\n",
    "\n",
    "    elif dataset_num == 7:\n",
    "        data_name = 'w8a'\n",
    "\n",
    "    elif dataset_num == 8:\n",
    "        data_name = 'covtype'\n",
    "\n",
    "    elif dataset_num == -1:\n",
    "        data_name = 'synthetic'\n",
    "\n",
    "    if (dataset_num >= 0):\n",
    "\n",
    "        # real data\n",
    "#         data = pickle.load(open(data_dir + data_name +'.pkl', 'rb'), encoding = \"latin1\")\n",
    "        data = load_libsvm(data_name, data_dir='./')\n",
    "\n",
    "        # load real dataset\n",
    "        A = data[0].toarray()\n",
    "\n",
    "        if dataset_num < 4:\n",
    "            y = data[1].toarray().ravel()\n",
    "        else:\n",
    "            y = data[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        A, y, w_true = create_dataset(n,d, margin, false_ratio)\n",
    "\n",
    "        # generate synthetic data - according to the BB paper\n",
    "#         x = np.random.randn(n, d)\n",
    "#         w_true = np.random.randn(d)\n",
    "#         y = np.sign(np.dot(x, w_true))\n",
    "\n",
    "    # subsample\n",
    "    if is_subsample == 1:\n",
    "        A = A[:n,:]\n",
    "        y = y[:n]\n",
    "\n",
    "    # split dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(A, y, test_size=test_prop, random_state=split_seed)\n",
    "\n",
    "    if is_kernelize == 1:\n",
    "        # Form kernel\n",
    "        A_train, A_test = kernelize(X_train, X_test, dataset_num, data_dir=data_dir)\n",
    "    else:\n",
    "        A_train = X_train\n",
    "        A_test = X_test\n",
    "\n",
    "    print('Loaded ', data_name ,' dataset.')\n",
    "\n",
    "    return A_train, y_train, A_test, y_test\n",
    "\n",
    "def kernelize(X, X_test, dataset_num, kernel_type=0, data_dir=\"./Data\"):\n",
    "\n",
    "    n = X.shape[0]\n",
    "\n",
    "    fname = data_dir + '/Kernel_' + str(n) + '_' + str(dataset_num) + '.p'\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "\n",
    "        print('Reading file ', fname)\n",
    "        X_kernel, X_test_kernel = pickle.load( open( fname, \"rb\" ) )\n",
    "\n",
    "    else:\n",
    "        if kernel_type == 0:\n",
    "            X_kernel = RBF_kernel(X, X)\n",
    "            X_test_kernel = RBF_kernel(X_test, X)\n",
    "            print('Formed the kernel matrix')\n",
    "\n",
    "        pickle.dump( (X_kernel, X_test_kernel) , open( fname, \"wb\" ) )\n",
    "\n",
    "    return X_kernel, X_test_kernel\n",
    "\n",
    "def RBF_kernel( A, B, sigma = 1.0 ):\n",
    "\n",
    "    distance_2 = np.square(  metrics.pairwise.pairwise_distances( X = A, Y = B, metric='euclidean'  )   )\n",
    "    K = np.exp( -1 * np.divide( distance_2, (2 * (sigma**2)) )  )\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def create_dataset(n,d,gamma = 0, false_ratio = 0):\n",
    "# create synthetic dataset using the python utility\n",
    "# X, y = datasets.make_classification(n_samples=n, n_features=d,n_informative = d, n_redundant = 0, class_sep = 2.0 )\n",
    "# convert into -1/+1\n",
    "# y = 2 * y - 1\n",
    "\n",
    "# create linearly separable dataset with margin gamma\n",
    "#w_star = np.random.random((d,1))\n",
    "    w_star = np.random.normal(0,1,(d,1))\n",
    "# normalize w_star\n",
    "    w_star = w_star / np.linalg.norm(w_star)\n",
    "\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    count = 0\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    y = np.zeros((n))\n",
    "\n",
    "    while(1):\n",
    "\n",
    "        x = np.random.normal( 1,1,(1,d) ) \n",
    "        # normalize x s.t. || x ||_2 = 1\n",
    "#         x = x / np.linalg.norm(x)\n",
    "\n",
    "        temp = np.dot( x, w_star )\n",
    "        margin = abs( temp )\n",
    "        sig = np.sign( temp )\n",
    "\n",
    "        if margin > gamma * np.linalg.norm(w_star):\n",
    "\n",
    "            if count % 2 == 0:\n",
    "\n",
    "                # generate positive\n",
    "                if sig > 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count, :] = -x\n",
    "                y[ count ] = + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                # generate negative\n",
    "                if sig < 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count,:] = -x\n",
    "                y[ count ] = - 1\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        if count == n:\n",
    "            break\n",
    "            \n",
    "    flip_ind = np.random.choice(n, int(n*false_ratio))\n",
    "    y[flip_ind] = -y[flip_ind]\n",
    "    \n",
    "    return X, y, w_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_minibatches(n, m, minibatch_size):\n",
    "    ''' Create m minibatches from the training set by sampling without replacement.\n",
    "        This function may sample the training set multiple times.\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        m: number of minibatches to generate\n",
    "        batch_size: size of the desired minibatches'''\n",
    "\n",
    "    k = math.ceil(m * minibatch_size / n)\n",
    "    batches = []\n",
    "    for i in range(k):\n",
    "        batches += minibatch_data(n, minibatch_size)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def minibatch_data(n, batch_size):\n",
    "    '''Splits training set into minibatches by sampling **without** replacement.\n",
    "    This isn't performant for large datasets (e.g. we should switch to PyTorch's streaming data loader eventually).\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        batch_size: size of the desired minibatches'''\n",
    "    # shuffle training set indices before forming minibatches\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    batches = []\n",
    "    num_batches = math.floor(n / batch_size)\n",
    "    # split the training set into minibatches\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        stop_index = (batch_num + 1) * batch_size\n",
    "\n",
    "        # create a minibatch\n",
    "        batches.append(indices[start_index:stop_index])\n",
    "\n",
    "    # generate a final, smaller batch if the batch_size doesn't divide evenly into n\n",
    "    if num_batches != math.ceil(n / batch_size):\n",
    "        batches.append(indices[stop_index:])\n",
    "\n",
    "    return batches\n",
    "\n",
    "def reset(model):\n",
    "    # reset the model\n",
    "    for param in model.parameters():\n",
    "        param.data = torch.zeros_like(param)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plotting(results, labels, max_epochs):\n",
    "    plt.figure()\n",
    "\n",
    "    offset = 0\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot( x, ( results[i,:] ), color = colors[i], label = labels[i] )\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_shaded_error_bars(results_mean, results_std, max_epochs, ylabel=\"Loss\", colors=None, labels=None):\n",
    "    fig = plt.figure()\n",
    "    offset = 0\n",
    "    if colors is None:\n",
    "        colors = ['r', 'b', 'g','k','cyan','lightgreen']\n",
    "    if labels is None:\n",
    "        labels = ['SVRG-BB', 'SVRG']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(results_mean.shape[0]):\n",
    "        plt.plot( x, ( results_mean[i,:] ), color = colors[i], label = labels[i] )\n",
    "        plt.fill_between(x, (results_mean[i,:] - results_std[i,:]), (results_mean[i,:] + results_std[i,:]), color=colors[i], alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    step_size = init_step_size\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            output += ', Num gradient evaluations: %d' % num_grad_evals\n",
    "            print(output)\n",
    "\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "        \n",
    "        \n",
    "        num_grad_evals = num_grad_evals + n\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "            \n",
    "            num_grad_evals = num_grad_evals + 1\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None, \n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size in the inner loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    AVERAGE_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "    MAX_ARMIJO_STEPS_REACHED = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        \n",
    "        start_step_size = max_step_size\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            f_x ,x_grad = closure(x, Di, labels_i)\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "            g = x_grad - x_tilde_grad + full_grad\n",
    "            g_norm = np.linalg.norm(g)\n",
    "            \n",
    "            step_size = start_step_size\n",
    "            found = False\n",
    "            for j in range(max_iter_armijo):\n",
    "                if closure(x - step_size*g, Di, labels_i)[0] > f_x - c*step_size*g_norm:\n",
    "                    step_size *= beta\n",
    "                    AVERAGE_ARMIJO_STEPS[k] += 1\n",
    "                else:\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            if found:\n",
    "                x -= step_size * g\n",
    "                if not reset_step_size:\n",
    "                    start_step_size = step_size\n",
    "            else:\n",
    "                MAX_ARMIJO_STEPS_REACHED[k]+=1\n",
    "                x -= 1e-6*g\n",
    "        AVERAGE_ARMIJO_STEPS[k] /= m\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Avg armijo steps: %.2e, Nb max iteration reached: %.2e' % \\\n",
    "                     (k, AVERAGE_ARMIJO_STEPS[k], MAX_ARMIJO_STEPS_REACHED[k])\n",
    "            print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, AVERAGE_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_armijo_outer_end(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size at the end of the outer loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    \n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    NB_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        \n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        norm_full_grad = np.linalg.norm(full_grad)\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                    (k, start_step_size, norm_full_grad)\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "        \n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = norm_full_grad\n",
    "        step_size = start_step_size\n",
    "        \n",
    "        found = False\n",
    "        for j in range(max_iter_armijo):\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                f_x ,x_grad = closure(x, Di, labels_i)\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "                g = x_grad - x_tilde_grad + full_grad\n",
    "                x -= step_size * g\n",
    "                \n",
    "            if closure(x, D, labels)[0] > loss - c*step_size*norm_full_grad:\n",
    "                step_size = beta*step_size\n",
    "                x = x_tilde.copy()\n",
    "                NB_ARMIJO_STEPS[k]+=1\n",
    "            else:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            if not reset_step_size:\n",
    "                start_step_size = step_size\n",
    "        if not found:\n",
    "            x -= 1e-6*full_grad\n",
    "        \n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Nb of Armijo steps: %.2e' % \\\n",
    "                         (k, NB_ARMIJO_STEPS[k])\n",
    "            print(output)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, NB_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_armijo_outer_beginning(closure, \n",
    "                batch_size, \n",
    "                D, \n",
    "                labels,\n",
    "                n, \n",
    "                d,\n",
    "                c,\n",
    "                beta,\n",
    "                max_iter_armijo,\n",
    "                max_step_size,\n",
    "                reset_step_size=False,\n",
    "                max_epoch=100, \n",
    "                m=0, \n",
    "                x0=None,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Armijo step size at the beginning of the outer loop for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "        max_iter_armijo: maximum number of Armijo iterations before falling back to small determined stepsize\n",
    "        reset_step_size: whether or not the stepsize should be reset to max_step_size at the next iteration\n",
    "        beta: multiplicative factor in Armijo line search (0 < beta < 1).\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    \n",
    "    start_step_size = max_step_size\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    NB_ARMIJO_STEPS = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        \n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        norm_full_grad = np.linalg.norm(full_grad)\n",
    "        \n",
    "        step_size = start_step_size\n",
    "        while closure(x - step_size*full_grad, D, labels)[0] > loss - c*step_size*norm_full_grad:\n",
    "            step_size *=beta\n",
    "            NB_ARMIJO_STEPS[k] +=1\n",
    "            if NB_ARMIJO_STEPS[k] == max_iter_armijo:\n",
    "                step_size = 1e-7\n",
    "                break\n",
    "        if not reset_step_size:\n",
    "            start_step_size = step_size\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e, Nb of Armijo steps: %d' % \\\n",
    "                    (k, step_size, norm_full_grad, NB_ARMIJO_STEPS[k])\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "        \n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = norm_full_grad  \n",
    "        \n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM, NB_ARMIJO_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True, adaptive_termination = False):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "    \n",
    "    num_grad_evals = 0\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        \n",
    "#         if k == 0:\n",
    "        \n",
    "#             Gk2 = 0\n",
    "#             step_size = init_step_size\n",
    "                        \n",
    "#             minibatches = make_minibatches(n, m, batch_size)\n",
    "#             for i in range(m):\n",
    "#                 # get the minibatch for this iteration\n",
    "#                 indices = minibatches[i]\n",
    "#                 Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "#                 # compute the gradients:\n",
    "#                 x_grad = closure(x, Di, labels_i)[1]\n",
    "\n",
    "#                 gk = x_grad \n",
    "\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "\n",
    "#                 x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "#                 num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "#                 if adaptive_termination == True:\n",
    "\n",
    "#                     if i == 0:\n",
    "#                         org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                     else:\n",
    "#                         temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "#                         # check condition to terminate inner loop\n",
    "#                         if temp/org < 1e-2:\n",
    "#                             print('Breaking from inner loop')                    \n",
    "#                             break                            \n",
    "        \n",
    "        if k >= 0:\n",
    "            print('Here')\n",
    "            loss, full_grad = closure(x, D, labels)\n",
    "            x_tilde = x.copy()\n",
    "\n",
    "            last_full_grad = full_grad\n",
    "            last_x_tilde = x_tilde\n",
    "\n",
    "            # initialize running sum of gradient norms\n",
    "            Gk2 = 0\n",
    "            step_size = init_step_size\n",
    "\n",
    "            if verbose:\n",
    "                output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                         (k, np.linalg.norm(full_grad))\n",
    "                output += ', Func. value: %e' % loss\n",
    "                output += ', Num gradient evaluations: %d' % num_grad_evals            \n",
    "                print(output)\n",
    "\n",
    "            if np.linalg.norm(full_grad) <= 1e-14:\n",
    "                return x, LOSS, GRAD_NORM\n",
    "\n",
    "            LOSS[k] = loss\n",
    "            GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "            num_grad_evals =  num_grad_evals  + n\n",
    "\n",
    "            # Create Minibatches:\n",
    "            minibatches = make_minibatches(n, m, batch_size)\n",
    "            for i in range(m):\n",
    "                # get the minibatch for this iteration\n",
    "                indices = minibatches[i]\n",
    "                Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "                # compute the gradients:\n",
    "                x_grad = closure(x, Di, labels_i)[1]\n",
    "                x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "                gk = x_grad - x_tilde_grad + full_grad\n",
    "\n",
    "                Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "#                 Gk2 = Gk2 + (np.linalg.norm(x_grad) ** 2)\n",
    "\n",
    "                x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "                num_grad_evals =  num_grad_evals  + 1\n",
    "\n",
    "                if adaptive_termination == True:\n",
    "\n",
    "                    if i == 0:\n",
    "                        org = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                    else:\n",
    "                        temp = (step_size / np.sqrt(Gk2))\n",
    "\n",
    "                        # check condition to terminate inner loop\n",
    "                        if temp/org < 1e-2 and i >=  int(n / batch_size):\n",
    "                            print(temp/org)\n",
    "                            print('Breaking from inner loop')                                \n",
    "                            break\n",
    "            \n",
    "            \n",
    "    return x, LOSS, GRAD_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def svrg_bb_ada(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "        \n",
    "        Gk2 = 0\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            gk = x_grad - x_tilde_grad + full_grad\n",
    "            \n",
    "            Gk2 = Gk2 + (np.linalg.norm(gk) ** 2)\n",
    "                        \n",
    "            x -= (step_size / np.sqrt(Gk2)) * (gk)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset options\n",
    "dataset_num = -1\n",
    "data_dir = './'\n",
    "\n",
    "is_subsample = 0\n",
    "is_kernelize = 0\n",
    "subsampled_n = -1\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(6162647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization options\n",
    "max_epochs = 15\n",
    "num_restarts = 1\n",
    "batch_size = 10\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loaded  synthetic  dataset.\n"
     ]
    }
   ],
   "source": [
    "# problem size when generating synthetic data\n",
    "if dataset_num == -1:\n",
    "    n, d = 10000, 20\n",
    "    false_ratio = 0.25\n",
    "    margin = 1e-6\n",
    "    print(is_kernelize)\n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num,n, d, margin, false_ratio)\n",
    "else:\n",
    "    if is_subsample == 1:\n",
    "        n = subsampled_n\n",
    "    else:\n",
    "        n = 0\n",
    "    if is_kernelize == 1:\n",
    "        d = n\n",
    "    else:\n",
    "        d = 0\n",
    "        \n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num, n,d, false_ratio, is_subsample, is_kernelize)\n",
    "    \n",
    "    if n == 0:\n",
    "        n = A.shape[0]\n",
    "    \n",
    "    \n",
    "#define the regularized losses we will use\n",
    "logistic_closure_l2 = make_closure(logistic_loss, 1/n)\n",
    "squared_hinge_closure_l2 = make_closure(squared_hinge_loss, 1/n)\n",
    "squared_closure_l2 = make_closure(squared_loss, 1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose experiment\n",
    "exp_num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if exp_num == 0:\n",
    "    print('Testing SVRG-BB:')\n",
    "    init_eta = 1e-3\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] = run_experiment(svrg_bb, logistic_closure_no_l2, A, y, A_test, y_test, init_eta, batch_size, max_epochs, num_restarts, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 1:\n",
    "    print('Testing robustness to step-size for SVRG:')    \n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = 'svrg', closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 2:\n",
    "    print('Testing robustness to size of inner loop for SVRG:')   \n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    init_eta = 1e-2\n",
    "    inner_loop_list = [0.5 , 1, 2, 5, 10]\n",
    "\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = len(inner_loop_list)\n",
    "\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    for inner_loop in inner_loop_list: \n",
    "        print('Begin to run SVRG with inner loop size = :', inner_loop)\n",
    "\n",
    "        m = int(inner_loop * n)\n",
    "\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = m, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(inner_loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 3:\n",
    "    \n",
    "    labels = []\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    step_sizes = [1e-4, 1e-2, 1, 100]\n",
    "        \n",
    "    num_variants = len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))     \n",
    "        \n",
    "    for init_eta in step_sizes:     \n",
    "  \n",
    "        print('Testing SVRG-Ada with step-size:', init_eta)\n",
    "    \n",
    "        x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = squared_closure_l2, \\\n",
    "                           X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                           init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                           m = 0, num_restarts = num_restarts, verbose=verbose) \n",
    "    \n",
    "        labels.append(['SVRG-Ada-', str(init_eta)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 4:\n",
    "    print('Testing SVRG vs SVRG-Ada vs SVRG-BB wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-3, 1e-2, 1e-1]\n",
    "    colors = ['r', 'b', 'g','r','b','g','r', 'b', 'g']\n",
    "\n",
    "    num_variants = 3 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = squared_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "#         # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "        \n",
    "        \n",
    "        # SVRG BB\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('BB' + str(init_eta))\n",
    "        \n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = 5 * int(n / batch_size), num_restarts = num_restarts, verbose=verbose,  adaptive_termination = True)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 5:\n",
    "    print('Testing SVRG-BB vs SVRG-BB-Ada wrt step-size robustness:')\n",
    "\n",
    "    count = 0\n",
    "    labels = []\n",
    "    step_sizes = [1e-2, 1e-1, 1]\n",
    "    colors = ['r', 'b', 'g','k','cyan','lightgreen','gray']\n",
    "\n",
    "    num_variants = 2 * len(step_sizes)\n",
    "\n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    closure = logistic_closure_l2\n",
    "\n",
    "    for init_eta in step_sizes: \n",
    "        print('Begin to run SVRG with step-size = :', init_eta)\n",
    "\n",
    "        # SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append(str(init_eta))\n",
    "\n",
    "        # Ada SVRG\n",
    "        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :] =\\\n",
    "        run_experiment(method_name = \"svrg_bb_ada\", closure = closure, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       init_step_size = init_eta, batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m =0, num_restarts = num_restarts, verbose=verbose)\n",
    "        count = count + 1\n",
    "        labels.append('Ada' + str(init_eta))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == 6:\n",
    "    print('Testing SVRG-Armijo:')\n",
    "    \n",
    "    num_variants = 1\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "\n",
    "    x_mean, results_mean2[0, :], results_std2[0, :], results2_mean2[0, :], results2_std2[0, :] =\\\n",
    "        run_experiment(method_name = \"svrg_armijo\", closure = logistic_closure_l2, \\\n",
    "                       X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                       batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                       m = n, num_restarts = num_restarts, verbose=True, init_step_size = 0.01, c=0.1, max_step_size = 0.1, \\\n",
    "                       beta = 0.7, max_iter_armijo=10, reset_step_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SVRG-Armijo, SVRG-Armijo with start of inner loop LS, SVRG-Armijo with end of inner loop LS with logistic loss\n",
      "Begin to run with batch_size: 10, c: 0.1, beta: 0.5, max_step_size: 0.1, reset_step_size:False\n",
      "Running Experiment:\n",
      "Epoch.: 0, Grad. norm: 3.00e-01, Func. value: 6.931472e-01\n",
      "Epoch.: 0, Avg armijo steps: 5.80e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 1, Grad. norm: 2.33e-01, Func. value: 6.853400e-01\n",
      "Epoch.: 1, Avg armijo steps: 5.90e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 2, Grad. norm: 2.33e-01, Func. value: 6.853400e-01\n",
      "Epoch.: 2, Avg armijo steps: 5.90e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 3, Grad. norm: 2.33e-01, Func. value: 6.853400e-01\n",
      "Epoch.: 3, Avg armijo steps: 5.50e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 4, Grad. norm: 2.10e-01, Func. value: 6.802383e-01\n",
      "Epoch.: 4, Avg armijo steps: 5.70e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 5, Grad. norm: 2.01e-01, Func. value: 6.759227e-01\n",
      "Epoch.: 5, Avg armijo steps: 5.40e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 6, Grad. norm: 1.90e-01, Func. value: 6.682259e-01\n",
      "Epoch.: 6, Avg armijo steps: 5.40e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 7, Grad. norm: 1.84e-01, Func. value: 6.638074e-01\n",
      "Epoch.: 7, Avg armijo steps: 5.30e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 8, Grad. norm: 1.84e-01, Func. value: 6.638074e-01\n",
      "Epoch.: 8, Avg armijo steps: 5.40e-03, Nb max iteration reached: 0.00e+00\n",
      "Epoch.: 9, Grad. norm: 1.71e-01, Func. value: 6.541306e-01\n",
      "Epoch.: 9, Avg armijo steps: 5.30e-03, Nb max iteration reached: 0.00e+00\n",
      "Restart 0, Test accuracy: 0.638000\n",
      "Running Experiment:\n",
      "Epoch.: 0, Step size: 1.00e-01, Grad. norm: 3.00e-01, Func. value: 6.931472e-01\n",
      "Epoch.: 0, Nb of Armijo steps: 0.00e+00\n",
      "Epoch.: 1, Step size: 1.00e-01, Grad. norm: 1.88e-01, Func. value: 5.918796e-01\n",
      "Epoch.: 1, Nb of Armijo steps: 0.00e+00\n",
      "Epoch.: 2, Step size: 1.00e-01, Grad. norm: 1.59e-02, Func. value: 5.854130e-01\n",
      "Epoch.: 2, Nb of Armijo steps: 1.00e+00\n",
      "Epoch.: 3, Step size: 5.00e-02, Grad. norm: 1.32e-03, Func. value: 5.853053e-01\n",
      "Epoch.: 3, Nb of Armijo steps: 4.00e+00\n",
      "Epoch.: 4, Step size: 3.13e-03, Grad. norm: 2.65e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 4, Nb of Armijo steps: 7.00e+00\n",
      "Epoch.: 5, Step size: 2.44e-05, Grad. norm: 1.09e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 5, Nb of Armijo steps: 4.00e+00\n",
      "Epoch.: 6, Step size: 1.53e-06, Grad. norm: 1.04e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 6, Nb of Armijo steps: 1.00e+00\n",
      "Epoch.: 7, Step size: 7.63e-07, Grad. norm: 1.02e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 7, Nb of Armijo steps: 1.00e+00\n",
      "Epoch.: 8, Step size: 3.81e-07, Grad. norm: 1.01e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 8, Nb of Armijo steps: 2.00e+00\n",
      "Epoch.: 9, Step size: 9.54e-08, Grad. norm: 1.00e-05, Func. value: 5.853045e-01\n",
      "Epoch.: 9, Nb of Armijo steps: 1.00e+00\n",
      "Restart 0, Test accuracy: 0.771000\n",
      "Running Experiment:\n",
      "Epoch.: 0, Step size: 1.00e-01, Grad. norm: 3.00e-01, Nb of Armijo steps: 0, Func. value: 6.931472e-01\n",
      "Epoch.: 1, Step size: 1.00e-01, Grad. norm: 1.88e-01, Nb of Armijo steps: 0, Func. value: 5.918796e-01\n",
      "Epoch.: 2, Step size: 2.27e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 42, Func. value: 5.854130e-01\n",
      "Epoch.: 3, Step size: 2.27e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Epoch.: 4, Step size: 2.27e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Epoch.: 5, Step size: 2.27e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Epoch.: 6, Step size: 1.14e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 1, Func. value: 5.854130e-01\n",
      "Epoch.: 7, Step size: 1.14e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Epoch.: 8, Step size: 1.14e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Epoch.: 9, Step size: 1.14e-14, Grad. norm: 1.59e-02, Nb of Armijo steps: 0, Func. value: 5.854130e-01\n",
      "Restart 0, Test accuracy: 0.771500\n"
     ]
    }
   ],
   "source": [
    "if exp_num == 7:\n",
    "    print(\"Testing SVRG-Armijo, SVRG-Armijo with start of inner loop LS, SVRG-Armijo with end of inner loop LS with logistic loss\")\n",
    "    batch_sizes = [10, 100, 1000]\n",
    "    c_values = [0.1, 0.5]\n",
    "    beta_values = [0.9]\n",
    "    max_step_sizes = [0.1, 1, 10]\n",
    "    reset_step_sizes = [True, False]\n",
    "    \n",
    "    count = 0\n",
    "    num_variants = 3*len(batch_sizes)*len(c_values)*len(beta_values)*len(max_step_sizes)*len(reset_step_sizes)\n",
    "    \n",
    "    results_mean = np.zeros((num_variants, max_epochs))\n",
    "    results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "    results2_mean = np.zeros((num_variants, max_epochs))\n",
    "    results2_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    results3_mean = np.zeros((num_variants, max_epochs))\n",
    "    results3_std = np.zeros((num_variants, max_epochs))\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for c in c_values:\n",
    "            for beta in beta_values:\n",
    "                for max_step_size in max_step_sizes:\n",
    "                    for reset_step_size in reset_step_sizes:\n",
    "                        print(\"Begin to run with batch_size: \" + str(batch_size) + \", c: \" + str(c)+\\\n",
    "                             \", beta: \" + str(beta) + \", max_step_size: \"+ str(max_step_size) +\\\n",
    "                              \", reset_step_size:\" + str(reset_step_size))\n",
    "                        \n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo\", closure = logistic_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=False, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        labels.append(\"svrg_armijo-batchsize:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size))\n",
    "                        count+=1\n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo_outer_end\", closure = logistic_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=False, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        labels.append(\"svrg_armijo_end-batchsize:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size))\n",
    "                        \n",
    "                        count+=1\n",
    "                        x_mean, results_mean[count, :], results_std[count, :], results2_mean[count, :], results2_std[count, :], results3_mean[count, :], results3_std[count, :] =\\\n",
    "                        run_experiment(method_name = \"svrg_armijo_outer_beginning\", closure = logistic_closure_l2, \\\n",
    "                                      X = A, y = y, X_test = A_test, y_test = y_test, \\\n",
    "                                      batch_size = batch_size, max_epochs = max_epochs, \\\n",
    "                                      m = n, num_restarts = num_restarts, verbose=False, c=c,\\\n",
    "                                      max_step_size = max_step_size, \\\n",
    "                                      beta = beta, max_iter_armijo=max_iter_armijo, reset_step_size=reset_step_size)\n",
    "                        \n",
    "                        labels.append(\"svrg_armijo_beginning-batchsize:\" + str(batch_size) + \"-c:\" + str(c)+\\\n",
    "                             \"-beta:\" + str(beta) + \"-max_step_size:\"+ str(max_step_size) +\\\n",
    "                              \"-reset_step_size:\" + str(reset_step_size))\n",
    "                        count+=1\n",
    "\n",
    "save = False\n",
    "if save:\n",
    "    save_dir = \"./tmp\"\n",
    "    np.savetxt(save_dir + \"loss_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"loss_std.out\", results_std, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"grad_norm_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"grad_norm_std.out\", results_std, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"armijo_steps_mean.out\", results_mean, delimiter= \",\")\n",
    "    np.savetxt(save_dir + \"armijo_steps_std.out\", results_std, delimiter= \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'b', 'g','r','b','g','r', 'b', 'g']\n",
    "labels = [\"1\", \"2\", \"3\"]\n",
    "fig = plot_shaded_error_bars(results_mean = results_mean, results_std = results_std, ylabel = 'Loss', max_epochs = max_epochs, colors = colors, labels = labels)\n",
    "plt.show()\n",
    "\n",
    "fig = plot_shaded_error_bars(results_mean = results2_mean, results_std = results2_std, ylabel = 'Grad_Norm', max_epochs = max_epochs, colors = colors, labels = labels)\n",
    "plt.show()\n",
    "\n",
    "#fig = plot_shaded_error_bars(results_mean = results3_mean, results_std = results3_std, ylabel = 'armijo_Steps', max_epochs = max_epochs, colors = colors, labels = labels)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./test\", results_mean, delimiter= \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
