{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn import datasets, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(method, closure, X, y, X_test, y_test, init_step_size=1e-3, batch_size=10, max_epochs=50, num_restarts=10, verbose=False, seed=9513451):\n",
    "    '''Run an experiment with multiple restarts and compute basic statistics from the runs.'''\n",
    "    # set the experiment seed\n",
    "    print(\"Running Experiment:\")\n",
    "    np.random.seed(seed)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "\n",
    "    n,d = X.shape\n",
    "    n_test = X_test.shape[0]\n",
    "    x_sum = np.zeros(d)\n",
    "\n",
    "    # do the restarts\n",
    "    for i in range(num_restarts):\n",
    "        x, loss_record, gradnorm_record = method(closure, batch_size, X, y, init_step_size, n, d, max_epoch=max_epochs, verbose=verbose)\n",
    "        x_sum += x\n",
    "        loss_results.append(loss_record)\n",
    "        gradnorm_results.append(gradnorm_record)\n",
    "\n",
    "        if verbose:\n",
    "            y_predict = np.sign(np.dot(X_test, x))\n",
    "            print('Restart %d, Test accuracy: %f' % (i, (np.count_nonzero(y_test == y_predict)*1.0 / n_test)))\n",
    "\n",
    "    # compute basic statistics from the runs\n",
    "    x_mean = x_sum / num_restarts\n",
    "\n",
    "    loss_results = np.stack(loss_results)\n",
    "    loss_std = loss_results.std(axis=0)\n",
    "    loss_mean = loss_results.mean(axis=0)\n",
    "\n",
    "    gradnorm_results = np.stack(gradnorm_results)\n",
    "    gradnorm_std = gradnorm_results.std(axis=0)\n",
    "    gradnorm_mean = gradnorm_results.mean(axis=0)\n",
    "\n",
    "    return x_mean, loss_mean, loss_std, gradnorm_mean, gradnorm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_closure(loss_fn, prior_prec=1e-2):\n",
    "    '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            loss_fn: the loss function to use (logistic loss, hinge loss, squared error, etc)\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: a closure fn for computing the loss and gradient. '''\n",
    "\n",
    "    def closure(w, X, y):\n",
    "        '''Computes loss and gradient of the loss w.r.t. w\n",
    "        Parameters:\n",
    "            w: weight vector\n",
    "            X: minibatch of input vectors\n",
    "            y: labels for the input vectors\n",
    "            prior_prec: precision of the Gaussian prior (pass 0 to avoid regularization)\n",
    "        Returns: (loss, gradient)'''\n",
    "        # change the Numpy Arrays into PyTorch Tensors\n",
    "        X = torch.tensor(X)\n",
    "        # Type of X is double, so y must be double.\n",
    "        y = torch.tensor(y, dtype=torch.double)\n",
    "        w = torch.tensor(w, requires_grad=True)\n",
    "\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(w, X, y) + (prior_prec / 2) * torch.sum(w**2)\n",
    "\n",
    "        # compute the gradient of loss w.r.t. w.\n",
    "        loss.backward()\n",
    "        # Put the gradient and loss back into Numpy.\n",
    "        grad = w.grad.detach().numpy()\n",
    "        loss = loss.item()\n",
    "\n",
    "        return loss, grad\n",
    "\n",
    "    return closure\n",
    "\n",
    "# PyTorch Loss Functions\n",
    "\n",
    "def logistic_loss(w, X, y):\n",
    "    ''' Logistic Loss'''\n",
    "    n,d = X.shape\n",
    "    return torch.mean(torch.log(1 + torch.exp(-torch.mul(y, torch.matmul(X, w)))))\n",
    "\n",
    "def squared_hinge_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Hinge Loss '''\n",
    "    return torch.mean((torch.max( torch.zeros(n,dtype=torch.double) , torch.ones(n,dtype=torch.double) - torch.mul(y, torch.matmul(X, w))))**2 )\n",
    "\n",
    "def squared_loss(w, X, y):\n",
    "    n,d = X.shape\n",
    "    '''Squared Loss'''\n",
    "    return torch.mean(( y - torch.matmul(X, w) )**2)\n",
    "\n",
    "### predefined loss closures ###\n",
    "\n",
    "logistic_closure_no_l2 = make_closure(logistic_loss, 0)\n",
    "logistic_closure_default_l2 = make_closure(logistic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(data_dir, dataset_num, n = 0, d = 0, is_subsample = 0, is_kernelize = 0, test_prop=0.2, split_seed=9513451):\n",
    "\n",
    "    if dataset_num == 0:\n",
    "        data_name = 'quantum'\n",
    "\n",
    "    elif dataset_num == 1:\n",
    "        data_name = 'rcv1'\n",
    "\n",
    "    elif dataset_num == 2:\n",
    "        data_name = 'protein'\n",
    "\n",
    "    elif dataset_num == 3:\n",
    "        data_name = 'news'\n",
    "\n",
    "    elif dataset_num == 4:\n",
    "        data_name = 'mushrooms'\n",
    "\n",
    "    elif dataset_num == 5:\n",
    "        data_name = 'splice'\n",
    "\n",
    "    elif dataset_num == 6:\n",
    "        data_name = 'ijcnn'\n",
    "\n",
    "    elif dataset_num == 7:\n",
    "        data_name = 'w8a'\n",
    "\n",
    "    elif dataset_num == 8:\n",
    "        data_name = 'covtype'\n",
    "\n",
    "    elif dataset_num == -1:\n",
    "        data_name = 'synthetic'\n",
    "\n",
    "    if (dataset_num >= 0):\n",
    "\n",
    "        # real data\n",
    "        data = pickle.load(open(data_dir + data_name +'.pkl', 'rb'), encoding = \"latin1\")\n",
    "\n",
    "        # load real dataset\n",
    "        A = data[0].toarray()\n",
    "\n",
    "        if dataset_num < 4:\n",
    "            y = data[1].toarray().ravel()\n",
    "        else:\n",
    "            y = data[1]\n",
    "\n",
    "    else:\n",
    "\n",
    "        A, y, w_true = create_dataset(n,d,0.5)\n",
    "\n",
    "        # generate synthetic data - according to the BB paper\n",
    "#         x = np.random.randn(n, d)\n",
    "#         w_true = np.random.randn(d)\n",
    "#         y = np.sign(np.dot(x, w_true))\n",
    "\n",
    "    # subsample\n",
    "    if is_subsample == 1:\n",
    "        A = A[:n,:]\n",
    "        y = y[:n]\n",
    "\n",
    "    # split dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(A, y, test_size=test_prop, random_state=split_seed)\n",
    "\n",
    "    if is_kernelize == 1:\n",
    "        # Form kernel\n",
    "        A_train, A_test = kernelize(X_train, X_test, dataset_num, data_dir=data_dir)\n",
    "    else:\n",
    "        A_train = X_train\n",
    "        A_test = X_test\n",
    "\n",
    "    print('Loaded ', data_name ,' dataset.')\n",
    "\n",
    "    return A_train, y_train, A_test, y_test\n",
    "\n",
    "def kernelize(X, X_test, dataset_num, kernel_type=0, data_dir=\"./Data\"):\n",
    "\n",
    "    n = X.shape[0]\n",
    "\n",
    "    fname = data_dir + '/Kernel_' + str(n) + '_' + str(dataset_num) + '.p'\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "\n",
    "        print('Reading file ', fname)\n",
    "        X_kernel, X_test_kernel = pickle.load( open( fname, \"rb\" ) )\n",
    "\n",
    "    else:\n",
    "        if kernel_type == 0:\n",
    "            X_kernel = RBF_kernel(X, X)\n",
    "            X_test_kernel = RBF_kernel(X_test, X)\n",
    "            print('Formed the kernel matrix')\n",
    "\n",
    "        pickle.dump( (X_kernel, X_test_kernel) , open( fname, \"wb\" ) )\n",
    "\n",
    "    return X_kernel, X_test_kernel\n",
    "\n",
    "def RBF_kernel( A, B, sigma = 1.0 ):\n",
    "\n",
    "    distance_2 = np.square(  metrics.pairwise.pairwise_distances( X = A, Y = B, metric='euclidean'  )   )\n",
    "    K = np.exp( -1 * np.divide( distance_2, (2 * (sigma**2)) )  )\n",
    "\n",
    "    return K\n",
    "\n",
    "\n",
    "def create_dataset(n,d,gamma):\n",
    "# create synthetic dataset using the python utility\n",
    "# X, y = datasets.make_classification(n_samples=n, n_features=d,n_informative = d, n_redundant = 0, class_sep = 2.0 )\n",
    "# convert into -1/+1\n",
    "# y = 2 * y - 1\n",
    "\n",
    "# create linearly separable dataset with margin gamma\n",
    "#w_star = np.random.random((d,1))\n",
    "    w_star = np.random.normal(0,1,(d,1))\n",
    "# normalize w_star\n",
    "    w_star = w_star / np.linalg.norm(w_star)\n",
    "\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    count = 0\n",
    "\n",
    "    X = np.zeros((n,d))\n",
    "    y = np.zeros((n))\n",
    "\n",
    "    while(1):\n",
    "\n",
    "        x = np.random.normal( 1,1,(1,d) )\n",
    "        # normalize x s.t. || x ||_2 = 1\n",
    "        x = x / np.linalg.norm(x)\n",
    "\n",
    "        temp = np.dot( x, w_star )\n",
    "        margin = abs( temp )\n",
    "        sig = np.sign( temp )\n",
    "\n",
    "        if margin > gamma * np.linalg.norm(w_star):\n",
    "\n",
    "            if count % 2 == 0:\n",
    "\n",
    "                # generate positive\n",
    "                if sig > 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count, :] = -x\n",
    "                y[ count ] = + 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                # generate negative\n",
    "                if sig < 0:\n",
    "                    X[count, :] = x\n",
    "                else:\n",
    "                    X[count,:] = -x\n",
    "                y[ count ] = - 1\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "        if count == n:\n",
    "            break\n",
    "\n",
    "    return X, y, w_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minibatches(n, m, minibatch_size):\n",
    "    ''' Create m minibatches from the training set by sampling without replacement.\n",
    "        This function may sample the training set multiple times.\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        m: number of minibatches to generate\n",
    "        batch_size: size of the desired minibatches'''\n",
    "\n",
    "    k = math.ceil(m * minibatch_size / n)\n",
    "    batches = []\n",
    "    for i in range(k):\n",
    "        batches += minibatch_data(n, minibatch_size)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def minibatch_data(n, batch_size):\n",
    "    '''Splits training set into minibatches by sampling **without** replacement.\n",
    "    This isn't performant for large datasets (e.g. we should switch to PyTorch's streaming data loader eventually).\n",
    "    Parameters:\n",
    "        n: the number of examples in the dataset\n",
    "        batch_size: size of the desired minibatches'''\n",
    "    # shuffle training set indices before forming minibatches\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    batches = []\n",
    "    num_batches = math.floor(n / batch_size)\n",
    "    # split the training set into minibatches\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        stop_index = (batch_num + 1) * batch_size\n",
    "\n",
    "        # create a minibatch\n",
    "        batches.append(indices[start_index:stop_index])\n",
    "\n",
    "    # generate a final, smaller batch if the batch_size doesn't divide evenly into n\n",
    "    if num_batches != math.ceil(n / batch_size):\n",
    "        batches.append(indices[stop_index:])\n",
    "\n",
    "    return batches\n",
    "\n",
    "def reset(model):\n",
    "    # reset the model\n",
    "    for param in model.parameters():\n",
    "        param.data = torch.zeros_like(param)\n",
    "    loss_results = []\n",
    "    gradnorm_results = []\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(results, max_epochs):\n",
    "    plt.figure()\n",
    "\n",
    "    offset = 0\n",
    "    colors = ['r', 'b', 'g','k','cyan']\n",
    "    labels = ['SVRG-BB', 'SVRG' ]\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot( x, ( results[i,:] ), color = colors[i], label = labels[i] )\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_shaded_error_bars(results_mean, results_std, max_epochs, label=\"Loss\", colors=None, labels=None):\n",
    "    fig = plt.figure()\n",
    "    offset = 0\n",
    "    if colors is None:\n",
    "        colors = ['r', 'b', 'g','k','cyan']\n",
    "    if labels is None:\n",
    "        labels = ['SVRG-BB', 'SVRG']\n",
    "\n",
    "    x = range(max_epochs)\n",
    "\n",
    "    for i in range(results_mean.shape[0]):\n",
    "        plt.plot( x, ( results_mean[i,:] ), color = colors[i], label = labels[i] )\n",
    "        plt.fill_between(x, (results_mean[i,:] - results_std[i,:]), (results_mean[i,:] + results_std[i,:]), color=colors[i], alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Number of Effective Passes')\n",
    "    plt.ylabel(label)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg_bb(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with Barzilai-Borwein step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: initial step size\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    step_size = init_step_size\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "        # estimate step size by BB method\n",
    "        if k > 0:\n",
    "            s = x_tilde - last_x_tilde\n",
    "            y = full_grad - last_full_grad\n",
    "            step_size = np.linalg.norm(s)**2 / np.dot(s, y) / m\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Step size: %.2e, Grad. norm: %.2e' % \\\n",
    "                     (k, step_size, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        # Add termination condition based on the norm of full gradient\n",
    "        # Without this, \"np.dot(s, y)\" can underflow and produce divide-by-zero errors.\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svrg(closure, batch_size, D, labels, init_step_size, n, d, max_epoch=100, m=0, x0=None, verbose=True):\n",
    "    \"\"\"\n",
    "        SVRG with fixed step size for solving finite-sum problems\n",
    "        Closure: a PyTorch-style closure returning the objective value and it's gradient.\n",
    "        batch_size: the size of minibatches to use.\n",
    "        D: the set of input vectors (usually X).\n",
    "        labels: the labels corresponding to the inputs D.\n",
    "        init_step_size: step-size to use\n",
    "        n, d: size of the problem\n",
    "    \"\"\"\n",
    "    step_size = init_step_size\n",
    "    if not isinstance(m, int) or m <= 0:\n",
    "        m = n\n",
    "        if verbose:\n",
    "            print('Info: set m=n by default')\n",
    "\n",
    "    if x0 is None:\n",
    "        x = np.zeros(d)\n",
    "    elif isinstance(x0, np.ndarray) and x0.shape == (d, ):\n",
    "        x = x0.copy()\n",
    "    else:\n",
    "        raise ValueError('x0 must be a numpy array of size (d, )')\n",
    "\n",
    "    LOSS = np.zeros((max_epoch))\n",
    "    GRAD_NORM = np.zeros((max_epoch))\n",
    "\n",
    "    for k in range(max_epoch):\n",
    "        loss, full_grad = closure(x, D, labels)\n",
    "        x_tilde = x.copy()\n",
    "\n",
    "        last_full_grad = full_grad\n",
    "        last_x_tilde = x_tilde\n",
    "\n",
    "        if verbose:\n",
    "            output = 'Epoch.: %d, Grad. norm: %.2e' % \\\n",
    "                     (k, np.linalg.norm(full_grad))\n",
    "            output += ', Func. value: %e' % loss\n",
    "            print(output)\n",
    "\n",
    "        if np.linalg.norm(full_grad) <= 1e-14:\n",
    "            return x, LOSS, GRAD_NORM\n",
    "\n",
    "        LOSS[k] = loss\n",
    "        GRAD_NORM[k] = np.linalg.norm(full_grad)\n",
    "\n",
    "        # Create Minibatches:\n",
    "        minibatches = make_minibatches(n, m, batch_size)\n",
    "        for i in range(m):\n",
    "            # get the minibatch for this iteration\n",
    "            indices = minibatches[i]\n",
    "            Di, labels_i = D[indices, :], labels[indices]\n",
    "\n",
    "            # compute the gradients:\n",
    "            x_grad = closure(x, Di, labels_i)[1]\n",
    "            x_tilde_grad = closure(x_tilde, Di, labels_i)[1]\n",
    "\n",
    "            x -= step_size * (x_grad - x_tilde_grad + full_grad)\n",
    "\n",
    "    return x, LOSS, GRAD_NORM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset options\n",
    "dataset_num = -1\n",
    "data_dir = './Data/'\n",
    "\n",
    "is_subsample = 0\n",
    "is_kernelize = 0\n",
    "subsampled_n = -1\n",
    "\n",
    "# set the seed for reproducibility\n",
    "np.random.seed(6162647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization options\n",
    "max_epochs = 20\n",
    "num_restarts = 1\n",
    "num_variants = 2\n",
    "batch_size = 100\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loaded  synthetic  dataset.\n"
     ]
    }
   ],
   "source": [
    "# problem size when generating synthetic data\n",
    "if dataset_num == -1:\n",
    "    n, d = 1000, 20\n",
    "    print(is_kernelize)\n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num,n, d)\n",
    "else:\n",
    "    if is_subsample == 1:\n",
    "        n = subsampled_n\n",
    "    else:\n",
    "        n = 0\n",
    "    if is_kernelize == 1:\n",
    "        d = n\n",
    "    else:\n",
    "        d = 0\n",
    "        \n",
    "    A, y, A_test, y_test = data_load(data_dir, dataset_num, n,d, is_subsample, is_kernelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = np.zeros((num_variants, max_epochs))\n",
    "results_std = np.zeros((num_variants, max_epochs))\n",
    "\n",
    "results2_mean = np.zeros((num_variants, max_epochs))\n",
    "results2_std = np.zeros((num_variants, max_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to run SVRG-BB:\n",
      "Running Experiment:\n",
      "Info: set m=n by default\n",
      "Epoch.: 0, Step size: 1.00e-03, Grad. norm: 3.18e-01, Func. value: 6.931472e-01\n",
      "Epoch.: 1, Step size: 1.09e-02, Grad. norm: 2.90e-01, Func. value: 6.194315e-01\n",
      "Epoch.: 2, Step size: 1.35e-02, Grad. norm: 1.34e-01, Func. value: 2.735625e-01\n",
      "Epoch.: 3, Step size: 2.50e-02, Grad. norm: 7.85e-02, Func. value: 1.601749e-01\n",
      "Epoch.: 4, Step size: 4.37e-02, Grad. norm: 4.49e-02, Func. value: 8.963916e-02\n",
      "Epoch.: 5, Step size: 7.49e-02, Grad. norm: 2.54e-02, Func. value: 4.955512e-02\n",
      "Epoch.: 6, Step size: 1.28e-01, Grad. norm: 1.44e-02, Func. value: 2.760178e-02\n",
      "Epoch.: 7, Step size: 2.21e-01, Grad. norm: 8.16e-03, Func. value: 1.556533e-02\n",
      "Epoch.: 8, Step size: 3.83e-01, Grad. norm: 4.66e-03, Func. value: 8.842998e-03\n",
      "Epoch.: 9, Step size: 6.67e-01, Grad. norm: 2.66e-03, Func. value: 5.045063e-03\n",
      "Epoch.: 10, Step size: 1.16e+00, Grad. norm: 1.52e-03, Func. value: 2.884843e-03\n",
      "Epoch.: 11, Step size: 2.03e+00, Grad. norm: 8.70e-04, Func. value: 1.651638e-03\n",
      "Epoch.: 12, Step size: 3.55e+00, Grad. norm: 4.98e-04, Func. value: 9.462260e-04\n",
      "Epoch.: 13, Step size: 6.21e+00, Grad. norm: 2.85e-04, Func. value: 5.422830e-04\n",
      "Epoch.: 14, Step size: 1.09e+01, Grad. norm: 1.63e-04, Func. value: 3.108334e-04\n",
      "Epoch.: 15, Step size: 1.90e+01, Grad. norm: 9.35e-05, Func. value: 1.781792e-04\n",
      "Epoch.: 16, Step size: 3.32e+01, Grad. norm: 5.36e-05, Func. value: 1.021391e-04\n",
      "Epoch.: 17, Step size: 5.80e+01, Grad. norm: 3.07e-05, Func. value: 5.854869e-05\n",
      "Epoch.: 18, Step size: 1.01e+02, Grad. norm: 1.76e-05, Func. value: 3.356035e-05\n",
      "Epoch.: 19, Step size: 1.77e+02, Grad. norm: 1.01e-05, Func. value: 1.923591e-05\n",
      "Restart 0, Test accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# # test SVRG-BB\n",
    "# x0 = np.random.rand(d)\n",
    "print('Begin to run SVRG-BB:')\n",
    "init_eta = 1e-3\n",
    "x_mean, results_mean[0, :], results_std[0, :], results2_mean[0, :], results2_std[0, :] = run_experiment(svrg_bb, logistic_closure_no_l2, A, y, A_test, y_test, init_eta, batch_size, max_epochs, num_restarts, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to run SVRG:\n",
      "Running Experiment:\n",
      "Info: set m=n by default\n",
      "Epoch.: 0, Grad. norm: 3.18e-01, Func. value: 6.931472e-01\n",
      "Epoch.: 1, Grad. norm: 2.90e-01, Func. value: 6.194315e-01\n",
      "Epoch.: 2, Grad. norm: 2.65e-01, Func. value: 5.579821e-01\n",
      "Epoch.: 3, Grad. norm: 2.43e-01, Func. value: 5.064780e-01\n",
      "Epoch.: 4, Grad. norm: 2.24e-01, Func. value: 4.630032e-01\n",
      "Epoch.: 5, Grad. norm: 2.07e-01, Func. value: 4.260135e-01\n",
      "Epoch.: 6, Grad. norm: 1.92e-01, Func. value: 3.942800e-01\n",
      "Epoch.: 7, Grad. norm: 1.79e-01, Func. value: 3.668303e-01\n",
      "Epoch.: 8, Grad. norm: 1.67e-01, Func. value: 3.428959e-01\n",
      "Epoch.: 9, Grad. norm: 1.57e-01, Func. value: 3.218677e-01\n"
     ]
    }
   ],
   "source": [
    "# # test SVRG-BB\n",
    "# x0 = np.random.rand(d)\n",
    "print('Begin to run SVRG:')\n",
    "init_eta = 1e-3\n",
    "x_mean, results_mean[1, :], results_std[1, :], results2_mean[1, :], results2_std[1, :] = run_experiment(svrg, logistic_closure_no_l2, A, y, A_test, y_test, init_eta, batch_size, max_epochs, num_restarts, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_shaded_error_bars(results_mean, results_std, max_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_shaded_error_bars(results2_mean, results2_std, max_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
